{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Closet_Index_Tracking/blob/master/File_00_NN_ext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o16Z8wP_6UDV"
      },
      "source": [
        "# **<h1 align=\"center\"><font color='#001ddd'> GLUCOSE PREDICTION DATASET**</font></h1>\n",
        "\n",
        "## **Dataset Description**\n",
        "The dataset is from a study that collected data from young adults in the UK with type 1 diabetes, who used a continuous glucose monitor (CGM), an insulin pump and a smartwatch. These devices collected blood glucose readings, insulin dosage, carbohydrate intake, and activity data. The data collected was aggregated to five-minute intervals and formatted into samples. Each sample represents a point in time and includes the aggregated five-minute intervals from the previous six hours. The aim is to predict the blood glucose reading an hour into the future, for each of these samples.\n",
        "\n",
        "The training set takes samples from the first three months of study data from nine of the participants and includes the future blood glucose value. These training samples appear in chronological order and overlap. The testing set takes samples from the remainder of the study period from fifteen of the participants (so unseen participants appear in the testing set). These testing samples do not overlap and are in a random order to avoid data leakage.\n",
        "\n",
        "**Complexities to be aware of:**\n",
        "\n",
        "This is medical data so there are missing values and noise in the data\n",
        "the participants did not all use the same device models (CGM, insulin pump and smartwatch) so there may be differences in the collection method of the data\n",
        "some participants in the test set do not appear in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--PrNXB8Ylys"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Connect to Colab:#\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install category-encoders\n",
        "!pip install optuna\n",
        "!pip install optuna-integration\n",
        "#!pip install scikit-learn==1.4\n",
        "!pip install catboost\n",
        "!pip install deeptables\n",
        "\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install keras-nlp\n",
        "!pip install BorutaShap\n",
        "!pip install scikit-lego\n",
        "!!pip install --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho3nCji3fPqQ"
      },
      "outputs": [],
      "source": [
        "folder_script = models_folders = \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose\"\n",
        "os.chdir(folder_script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syKhpAm8jJZe"
      },
      "outputs": [],
      "source": [
        "from category_encoders.cat_boost import CatBoostEncoder\n",
        "from category_encoders.wrapper import PolynomialWrapper\n",
        "from category_encoders.count import CountEncoder\n",
        "\n",
        "# Setup notebook\n",
        "from pathlib import Path\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pickle import load, dump\n",
        "import json\n",
        "import joblib\n",
        "#import calplot as cal\n",
        "import missingno as msno\n",
        "import category_encoders as ce\n",
        "\n",
        "# Graphic Libraries:\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "\n",
        "# Bloomberg\n",
        "#from xbbg import blp\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "# Time Management\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BMonthEnd, QuarterEnd\n",
        "import datetime\n",
        "from pandas.tseries.offsets import BDay # BDay is business day, not birthday...\n",
        "import datetime as dt\n",
        "import click\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import string\n",
        "\n",
        "from ipywidgets import AppLayout\n",
        "from ipywidgets import Dropdown, Layout, HTML, AppLayout, VBox, Label, HBox, BoundedFloatText, interact, Output\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Reshape\n",
        "from keras.models import Model\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
        "\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, cross_validate, GroupKFold, GridSearchCV, RepeatedStratifiedKFold, cross_val_predict\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   LabelEncoder,\n",
        "                                   QuantileTransformer,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "# metrics\n",
        "import sklearn\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             root_mean_squared_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss,\n",
        "                             make_scorer)\n",
        "\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  HuberRegressor,\n",
        "                                  TweedieRegressor,\n",
        "                                  QuantileRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  TheilSenRegressor,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              AdaBoostClassifier,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              GradientBoostingClassifier,\n",
        "                              StackingRegressor,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              HistGradientBoostingRegressor,\n",
        "                              ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
        "\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import itertools\n",
        "import warnings\n",
        "import logging\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from pylab import rcParams\n",
        "import scipy.stats as ss\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "#plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Setting rc parameters in seaborn for plots and graphs-\n",
        "# Reference - https://matplotlib.org/stable/tutorials/introductory/customizing.html:-\n",
        "# To alter this, refer to matplotlib.rcParams.keys()\n",
        "\n",
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.5,\n",
        "         'grid.alpha'           :0.5,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : 'black',\n",
        "         'axes.titlesize'       : 12,\n",
        "         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "# Set Style\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "G8YvvLtMYTBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install graphviz\n",
        "import os\n",
        "os.environ[\"PATH\"] += os.pathsep + '/usr/bin/dot'"
      ],
      "metadata": {
        "id": "gxySMoesORTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 Upload Data"
      ],
      "metadata": {
        "id": "eIq4n5QIJtVc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaMB0Ku9izBm"
      },
      "source": [
        "## 1.1 Functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXVCNFXEiysu"
      },
      "outputs": [],
      "source": [
        "def encode_target(y_train, y_test, encoder_type='label', enc_strategy=False):\n",
        "    \"\"\"\n",
        "    Encodes the target columns in the training and testing data\n",
        "    using the specified encoder type.\n",
        "\n",
        "    Parameters:\n",
        "    y_train (pd.Series or pd.DataFrame): Training target data.\n",
        "    y_test (pd.Series or pd.DataFrame): Testing target data.\n",
        "\n",
        "    Returns:\n",
        "    y_train_encoded (pd.Series): Encoded training target data.\n",
        "    y_test_encoded (pd.Series): Encoded testing target data.\n",
        "    \"\"\"\n",
        "\n",
        "    if encoder_type == 'label':\n",
        "        encoder = LabelEncoder()\n",
        "        y_train_encoded = encoder.fit_transform(y_train)\n",
        "        y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "        y_train_encoded = pd.Series(y_train_encoded, index=y_train.index, name=\"Target\")\n",
        "        y_test_encoded = pd.Series(y_test_encoded, index=y_test.index, name=\"Target\")\n",
        "\n",
        "\n",
        "    elif encoder_type == 'onehot':\n",
        "        y_train_ = y_train.values.reshape(-1, 1)\n",
        "        y_test_ = y_test.values.reshape(-1, 1)\n",
        "\n",
        "        encoder = OneHotEncoder(sparse_output=False)\n",
        "        y_train_encoded = encoder.fit_transform(y_train_)\n",
        "        y_test_encoded = encoder.transform(y_test_)\n",
        "\n",
        "        y_train_encoded = pd.DataFrame(y_train_encoded, index=y_train.index)\n",
        "        y_test_encoded = pd.DataFrame(y_test_encoded, index=y_test.index)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid encoder_type. Currently supported: 'label'.\")\n",
        "\n",
        "    if enc_strategy:\n",
        "        return y_train_encoded, y_test_encoded, encoder\n",
        "\n",
        "    else:\n",
        "        return y_train_encoded, y_test_encoded\n",
        "\n",
        "def encode_data(X_train, X_test, encoder_type='label', columns=None, map=None):\n",
        "    \"\"\"\n",
        "    Encodes the training and testing data using the specified encoder type.\n",
        "\n",
        "    Parameters:\n",
        "    X_train (pd.DataFrame): Training data.\n",
        "    X_test (pd.DataFrame): Testing data.\n",
        "    encoder_type (str): Type of encoder ('label' or 'onehot'). Default is 'label'.\n",
        "    columns (list): List of columns to encode. If None, all object type columns are encoded.\n",
        "\n",
        "    Returns:\n",
        "    X_train_encoded (pd.DataFrame): Encoded training data.\n",
        "    X_test_encoded (pd.DataFrame): Encoded testing data.\n",
        "    \"\"\"\n",
        "\n",
        "    if columns is None:\n",
        "        # Default to all object type columns if no columns are specified\n",
        "        columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    X_train_encoded = X_train.copy()\n",
        "    X_test_encoded = X_test.copy()\n",
        "\n",
        "    if encoder_type == 'label':\n",
        "        for col in columns:\n",
        "            le = LabelEncoder()\n",
        "            X_train_encoded[col] = le.fit_transform(X_train[col])\n",
        "            X_test_encoded[col] = le.transform(X_test[col])\n",
        "\n",
        "    elif encoder_type == 'onehot':\n",
        "        for col in columns:\n",
        "            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first')\n",
        "            # Fit the encoder on the training data and transform both training and test data\n",
        "            encoded_train = ohe.fit_transform(X_train[[col]])\n",
        "            encoded_test = ohe.transform(X_test[[col]])\n",
        "\n",
        "            # Create a DataFrame with the encoded data\n",
        "            encoded_train_df = pd.DataFrame(encoded_train, columns=ohe.get_feature_names_out([col]))\n",
        "            encoded_test_df = pd.DataFrame(encoded_test, columns=ohe.get_feature_names_out([col]))\n",
        "\n",
        "            # Concatenate the new columns to the original dataframes and drop the original columns\n",
        "            X_train_encoded = pd.concat([X_train_encoded.drop(col, axis=1), encoded_train_df], axis=1)\n",
        "            X_test_encoded = pd.concat([X_test_encoded.drop(col, axis=1), encoded_test_df], axis=1)\n",
        "\n",
        "    elif encoder_type == 'count_encoder':\n",
        "\n",
        "          for col in columns:\n",
        "\n",
        "                target_encoder = CountEncoder(cols=columns)\n",
        "                X_train_encoded = target_encoder.fit_transform(X_train_encoded)\n",
        "                X_test_encoded = target_encoder.transform(X_test_encoded)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid encoder_type. Currently supported: 'label', 'onehot', 'target_encoder'.\")\n",
        "\n",
        "    return X_train_encoded, X_test_encoded\n",
        "\n",
        "def plot_training_session(history):\n",
        "  # Plot training and validation loss scores\n",
        "  # against the number of epochs.\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.plot(history.history['loss'], label='Train')\n",
        "  plt.plot(history.history['val_loss'], label='Validation')\n",
        "  plt.grid(linestyle='--')\n",
        "  plt.ylabel('val_loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.title('Train-Validation Scores', pad=13)\n",
        "  plt.legend(loc='upper right');\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5NjoRTdyVHe"
      },
      "source": [
        "## **1.2 Importing the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEYVvvGmeG2W"
      },
      "source": [
        "### **1.2.1 Files**\n",
        "* activities.txt - a list of activity names that appear in the activity-X:XX columns\n",
        "* sample_submission.csv - a sample submission file in the correct format\n",
        "* test.csv - the test set\n",
        "* train.csv - the training set\n",
        "\n",
        "## **Columns**\n",
        "* train.csv:\n",
        "    * **id - row id** consisting of participant number and a count for that participant\n",
        "    * **p_num** - participant number\n",
        "    * **time** - time of day in the format HH:MM:SS\n",
        "    * **bg-X:XX** - blood glucose reading in mmol/L, X:XX(H:SS) time in the past (e.g. bg-2:35, would be the blood glucose reading from 2 hours and 35 minutes before the time value for that row), recorded by the continuous glucose monitor\n",
        "    * **insulin-X:XX** - total insulin dose received in units in the last 5 minutes, X:XX(H:SS) time in the past (e.g. insulin-2:35, would be the total insulin dose received between 2 hours and 40 minutes and 2 hours and 35 minutes before the time value for that row), recorded by the insulin pump\n",
        "    * **carbs-X:XX** - total carbohydrate value consumed in grammes in the last 5 minutes, X:XX(H:SS) time in the past (e.g. carbs-2:35, would be the total carbohydrate value consumed between 2 hours and 40 minutes and 2 hours and 35 minutes before the time value for that row), recorded by the participant\n",
        "    * **hr-X:XX** - mean heart rate in beats per minute in the last 5 minutes, X:XX(H:SS) time in the past (e.g. hr-2:35, would be the mean heart rate between 2 hours and 40 minutes and 2 hours and 35 minutes before the time value for that row), recorded by the smartwatch\n",
        "    * **steps-X:XX** - total steps walked in the last 5 minutes, X:XX(H:SS) time in the past (e.g. * steps-2:35, would be the total steps walked between 2 hours and 40 minutes and 2 hours and 35 minutes before the time value for that row), recorded by the smartwatch\n",
        "    * **cals-X:XX** - total calories burnt in the last 5 minutes, X:XX(H:SS) time in the past (e.g. cals-2:35, would be the total calories burned between 2 hours and 40 minutes and 2 hours and 35 minutes before the time value for that row), calculated by the smartwatch\n",
        "    * **activity-X:XX** - self-declared activity performed in the last 5 minutes, X:XX(H:SS) time in the past (e.g. activity-2:35, would show a string name of the activity performed between 2 hours and 40 minutes and 2 hours and 35 minutes before the time value for that row), set on the smartwatch\n",
        "    * **bg+1:00** - blood glucose reading in mmol/L an hour in the future, this is the value you will be predicting (not provided in test.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fiLvkXQuzJ3"
      },
      "outputs": [],
      "source": [
        "ext_data=True\n",
        "\n",
        "if ext_data==False:\n",
        "  df_train=pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/train_cluster.csv\", index_col=0)\n",
        "  df_test=pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/test_cluster.csv\", index_col=0)\n",
        "\n",
        "  df_train_scaled=pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/X_train_scaled.csv\", index_col=0)\n",
        "  df_test.shape\n",
        "\n",
        "if ext_data==True:\n",
        "  df_train = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/train_cluster_enc_final_ext.csv\", index_col=0)\n",
        "  df_test = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/test_cluster_enc_final_ext.csv\", index_col=0)\n",
        "\n",
        "#  df_train_scaled=pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/X_train_scaled.csv\", index_col=0)\n",
        "  print(df_test.shape,df_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIdNSqVxOY7o"
      },
      "outputs": [],
      "source": [
        "df_train.p_num.value_counts()/df_train.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.p_num = df_train.p_num.astype(\"str\")\n",
        "df_test.p_num = df_test.p_num.astype(\"str\")"
      ],
      "metadata": {
        "id": "7CM4Y6Mr5CuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_train.groupby([\"cluster\"])[\"p_num\"].count()"
      ],
      "metadata": {
        "id": "azNdRmatWeUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8OmmGKk8KPX"
      },
      "outputs": [],
      "source": [
        "#df_train.groupby([\"cluster\",\"p_num\"])[\"p_num\"].count()[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nJ0_v3bl0_s"
      },
      "outputs": [],
      "source": [
        "df_train.head()\n",
        "df_train.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SCALE FEATURES**"
      ],
      "metadata": {
        "id": "s9ClCj7Xr8f7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z8CiYrCCuNQ"
      },
      "outputs": [],
      "source": [
        "float_feat = ['PC_1', 'PC_2', 'PC_3', 'PC_4', 'enc_08_v6', 'enc_07_v7', 'enc_03_v7', 'enc_01_v5',\n",
        "              'enc_02_v5', 'enc_03_v5', 'enc_04_v5', 'enc_05_v5', 'enc_06_v5', 'enc_07_v5', 'enc_08_v5']\n",
        "\n",
        "scaler_float = StandardScaler()\n",
        "\n",
        "df_train[float_feat] = scaler_float.fit_transform(df_train[float_feat])\n",
        "df_test[float_feat] = scaler_float.transform(df_test[float_feat])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FXSnbRYnNg9"
      },
      "source": [
        "Sub-dataset are created for each main set of features to inpute missing values:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.shape)\n",
        "display(df_train.groupby(\"p_num\")[\"p_num\"].count())\n",
        "#display(dict(df_test.groupby(\"p_num\")[\"p_num\"].count()))\n",
        "#display(df_train[df_train[\"p_num\"]==\"p01\"].head())\n",
        "#display(df_train[df_train[\"p_num\"]==\"p02\"].head())\n",
        "\n",
        "vocabulary_pnum = list(dict(df_train.groupby(\"p_num\")[\"p_num\"].count()).keys())\n",
        "len(vocabulary_pnum)"
      ],
      "metadata": {
        "id": "IYaD4UwO-dDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Features:"
      ],
      "metadata": {
        "id": "2eTNy49rV95Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df_train.columns==df_test.columns).all()"
      ],
      "metadata": {
        "id": "bm92GR08f5J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define Features Datatypes:"
      ],
      "metadata": {
        "id": "LgeSuWl0YGQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_fields = list(df_test.columns)\n",
        "df_train.info()"
      ],
      "metadata": {
        "id": "9_BgQr9HT2B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train.copy()\n",
        "X_test = df_test.copy()"
      ],
      "metadata": {
        "id": "EY2S5D2R_QWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_cols = X_train.select_dtypes(include=['int']).columns.tolist()\n",
        "float_cols = X_train.select_dtypes(include=['float']).columns.tolist()\n",
        "obj_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "X_train[int_cols] = X_train[int_cols].astype(\"int32\")\n",
        "X_train[float_cols] = X_train[float_cols].astype(\"float32\")\n",
        "X_train[obj_cols] = X_train[obj_cols].astype(\"string\")\n",
        "\n",
        "X_test[int_cols] = X_test[int_cols].astype(\"int32\")\n",
        "X_test[float_cols] = X_test[float_cols].astype(\"float32\")\n",
        "X_test[obj_cols] = X_test[obj_cols].astype(\"string\")"
      ],
      "metadata": {
        "id": "ouO2Yf1qpNiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "2HEN6SUz_TL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "yb2Obgb13vdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.0 Neural Network Preparation:**"
      ],
      "metadata": {
        "id": "UG_hd3oiJkiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **FUNCTIONS**"
      ],
      "metadata": {
        "id": "AXJUGUchhk-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(input_tensor, filters, kernel_size, strides=1):\n",
        "    x = layers.Conv1D(filters, kernel_size, strides=strides, padding='same')(input_tensor)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.SpatialDropout1D(0.2)(x)\n",
        "\n",
        "    # Residual connection\n",
        "    if strides > 1 or input_tensor.shape[-1] != filters:\n",
        "        residual = layers.Conv1D(filters, 1, strides=strides, padding='same')(input_tensor)\n",
        "    else:\n",
        "        residual = input_tensor\n",
        "\n",
        "    x = layers.add([x, residual])\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    return x\n",
        "\n",
        "def dense_block(cells, dropout=0.3,activation=\"selu\", reg=0.0, name=None, normalization_type='batch'):\n",
        "  if activation == \"selu\":\n",
        "    block = keras.Sequential(name=name)\n",
        "    block.add(Dense(cells,\n",
        "                    activity_regularizer=keras.regularizers.l2(reg),\n",
        "                    name=f\"{name}_dense\",\n",
        "                    kernel_initializer=\"lecun_normal\"))\n",
        "\n",
        "    if normalization_type == 'batch':\n",
        "      block.add(layers.BatchNormalization(name=f\"{name}_bn\"))\n",
        "    elif normalization_type == 'layer':\n",
        "      block.add(layers.LayerNormalization(name=f\"{name}_ln\"))\n",
        "    else:\n",
        "      raise ValueError(\"Invalid normalization_type. Choose 'batch' or 'layer'.\")\n",
        "\n",
        "    block.add(layers.Activation(activation, name=f\"{name}_activ\"))\n",
        "    block.add(layers.AlphaDropout(dropout, name=f\"{name}_do_alpha\"))\n",
        "\n",
        "  else:\n",
        "    block = keras.Sequential(name=name)\n",
        "    block.add(Dense(cells,\n",
        "                    activity_regularizer=keras.regularizers.l2(reg),\n",
        "                    name=f\"{name}_dense\",\n",
        "                    kernel_initializer=\"he_normal\"))\n",
        "\n",
        "    if normalization_type == 'batch':\n",
        "      block.add(layers.BatchNormalization(name=f\"{name}_bn\"))\n",
        "    elif normalization_type == 'layer':\n",
        "      block.add(layers.LayerNormalization(name=f\"{name}_ln\"))\n",
        "    else:\n",
        "      raise ValueError(\"Invalid normalization_type. Choose 'batch' or 'layer'.\")\n",
        "\n",
        "    block.add(layers.Activation(activation, name=f\"{name}_activ\"))\n",
        "    block.add(layers.Dropout(dropout, name=f\"{name}_do\"))\n",
        "\n",
        "  return block\n",
        "\n",
        "def residual_block_dense(input_tensor, units, normalization_type='batch', dropout_rate=0.2,activation='relu', name=\"block\",reg=0.0):\n",
        "    \"\"\"\n",
        "    Creates a residual block with dense layers.\n",
        "\n",
        "    Args:\n",
        "      input_tensor: Input tensor to the block.\n",
        "      units: Number of units in the dense layer.\n",
        "      normalization_type: Type of normalization to use ('batch' or 'layer').\n",
        "      dropout_rate: Dropout rate for the dropout layer.\n",
        "\n",
        "    Returns:\n",
        "      Output tensor of the block.\n",
        "    \"\"\"\n",
        "\n",
        "    x = layers.Dense(units, activation=activation,name=f\"dense_01_{name}\",activity_regularizer=keras.regularizers.l2(reg))(input_tensor)\n",
        "    if normalization_type == 'batch':\n",
        "        x = layers.BatchNormalization(name=f\"bn_01_{name}\")(x)\n",
        "    elif normalization_type == 'layer':\n",
        "        x = layers.LayerNormalization(name=f\"ln_01_{name}\")(x)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid normalization_type. Choose 'batch' or 'layer'.\")\n",
        "    x = layers.Dropout(dropout_rate,name=f\"do_01_{name}\")(x)\n",
        "\n",
        "    # Residual connection\n",
        "    if input_tensor.shape[-1] != units:\n",
        "        residual = layers.Dense(units,name=f\"dense_02_{name}\")(input_tensor)\n",
        "    else:\n",
        "        residual = input_tensor\n",
        "\n",
        "    x = layers.add([x, residual],name=f\"add_01_{name}\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "S7kURNoAheCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 MODEL 01**"
      ],
      "metadata": {
        "id": "XhpN5CyY8FpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Create Dataloader:"
      ],
      "metadata": {
        "id": "R0DhCJeVotb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "1I2oq5LHqUs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_to_dataset(dataframe_, shuffle=False, batch_size=32):\n",
        "    dataframe = dataframe_.copy()\n",
        "    target = dataframe[\"bg+1:00\"]\n",
        "    dataframe = dataframe.drop(columns=[\"bg+1:00\"])\n",
        "    tab_data = dataframe.drop(columns=[\"p_num\"])\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((dataframe[\"p_num\"].values,  # First input\n",
        "                                              tab_data.values), # Second input\n",
        "                                              target))\n",
        "\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(batch_size)\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "r4eag5z1pJhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataframe_to_dataset(X_train,batch_size=1)"
      ],
      "metadata": {
        "id": "IlhbQu21y0um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TEST THE DATALOADER:**"
      ],
      "metadata": {
        "id": "vjaS-Nx74rFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (x0, x1), y in train_ds.take(1):\n",
        "    print(x0.shape)\n",
        "    print(x1.shape)\n",
        "    print(y.shape)"
      ],
      "metadata": {
        "id": "8PK43bpz2qdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "AuvXa0GH40YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.3 Create Models:**"
      ],
      "metadata": {
        "id": "u63MJMKkTDnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_v0(vocabulary=vocabulary_pnum,\n",
        "                    dense_layers=[256,128,128,64],\n",
        "                    dropout=0.3,\n",
        "                    activation=\"selu\",\n",
        "                    reg=0.0,\n",
        "                    lr=0.001,\n",
        "                    noise=0.05,\n",
        "                    summary=False):\n",
        "\n",
        "  # --- Tabular Data Processing ---\n",
        "  # Input layer for tabular data as a dictionary\n",
        "\n",
        "  pnum_input = keras.Input(shape=(1,), name=\"pnum_input\", dtype=\"string\")\n",
        "\n",
        "  # Lookup Layer for the p_num:\n",
        "  lookup = layers.StringLookup(vocabulary=vocabulary,mask_token=None,num_oov_indices=1,\n",
        "                output_mode=\"int\", name=\"lookup_pnum\")\n",
        "  pnum_encoded = lookup(pnum_input)\n",
        "\n",
        "  # Embedding layers for hour and minute\n",
        "  pnum_encoded_embedding = layers.Embedding(input_dim=len(vocabulary)+1,\n",
        "                                            output_dim=5,\n",
        "                                            name=\"embed_pnum\")(pnum_encoded)\n",
        "  # Flatten the embeddings\n",
        "  pnum_flat = layers.Flatten()(pnum_encoded_embedding)\n",
        "\n",
        "  # Input layer for Tabular Data\n",
        "  tab_data = keras.Input(shape=(15,), name=\"tab_input\")\n",
        "\n",
        "  # Concatenate all tabular features\n",
        "  x = layers.concatenate([tab_data, pnum_flat])\n",
        "  x = layers.BatchNormalization()(x)\n",
        "\n",
        "  x = layers.GaussianNoise(stddev=noise, seed=42, name=\"gn_00\")(x)\n",
        "  tabular_output = x\n",
        "  tabular_output = layers.Dropout(dropout, name=\"bb_00\")(tabular_output)\n",
        "\n",
        "  # Dense layers for tabular data\n",
        "  for num, den in enumerate(dense_layers):\n",
        "    tabular_output = dense_block(den,dropout=dropout,activation=activation, reg=reg, name=f\"block_{num}\")(tabular_output)\n",
        "\n",
        "\n",
        "  tabular_output = layers.concatenate([x, tabular_output])\n",
        "  # --- Output Layer ---\n",
        "  # Final dense layer for prediction\n",
        "  output = layers.Dense(1, name=\"output\")(tabular_output)\n",
        "\n",
        "  # --- Create and Compile the Model ---\n",
        "  # Create the model\n",
        "  model = keras.Model(inputs=[pnum_input, tab_data], outputs=output)\n",
        "  optimizer= keras.optimizers.Adam(learning_rate=lr)\n",
        "  metric = RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
        "  metric_mae = MeanAbsoluteError(name=\"mae\", dtype=None)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[metric,metric_mae])\n",
        "\n",
        "  # Print model summary\n",
        "  if summary==True:\n",
        "    model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_model_v0(vocabulary=vocabulary_pnum,summary=True)"
      ],
      "metadata": {
        "id": "yAHTpsL-STbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True, rankdir=\"LR\", expand_nested=True, show_layer_activations=True)"
      ],
      "metadata": {
        "id": "j9ga93XT9a_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Training Functions:"
      ],
      "metadata": {
        "id": "1BAk6TqcIpqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function:"
      ],
      "metadata": {
        "id": "DXquRNpNdBdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(X_train, X_test, model_constructor, best_params, experiment_name=\"baseline_nn_ext\", rs=42, target=\"bg+1:00\",\n",
        "                   batch_size=64, num_epochs=200, learning_rate=0.001, n_splits = 5, n_repeats = 5, vocabulary=vocabulary_pnum,\n",
        "                   patience=11,patience_reduce=5):\n",
        "\n",
        "  test_predictions = np.zeros((len(X_test),1))\n",
        "  test_results_df = pd.DataFrame(index=X_test.index, columns=list(range(n_repeats*n_splits)))\n",
        "  train_results_df = pd.DataFrame(index=X_train.index, columns=list(range(n_repeats*n_splits)))\n",
        "\n",
        "  all_mse = []\n",
        "  all_rmse = []\n",
        "\n",
        "  rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=36851234)\n",
        "\n",
        "  for i, (train_index, val_index) in enumerate(rskf.split(X_train, X_train[\"p_num\"])):\n",
        "      print(f\"\\nRunning CV {i}\\n\")\n",
        "      X, val_X = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "      y, val_y = X_train[target].iloc[train_index], X_train[target].iloc[val_index]\n",
        "      X_test = X_test.copy()\n",
        "      #################################################################### Prepare Datasets loaders:\n",
        "      train_dataset = dataframe_to_dataset(X, batch_size=batch_size, shuffle=True)\n",
        "      valid_dataset = dataframe_to_dataset(val_X, batch_size=batch_size, shuffle=False)\n",
        "      test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "      ##################################################################### Relevant Folders\n",
        "      folders_experiment = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/Glucose/{experiment_name}_{i}/\"\n",
        "      ##################################################################### Generate and Fit Model\n",
        "      # Callbacks:\n",
        "      checkpoint_filepath = folders_experiment + f'checkpoint/{experiment_name}_{i}.weights.h5'\n",
        "\n",
        "      # Generate the Model:\n",
        "      model = model_constructor(vocabulary=vocabulary,\n",
        "                                lr = learning_rate,\n",
        "                                **best_params)\n",
        "      if i>=0:\n",
        "        print(\"Start training the model...\")\n",
        "\n",
        "        history = model.fit(train_dataset,\n",
        "                            epochs=num_epochs,\n",
        "                            callbacks=[keras.callbacks.EarlyStopping(monitor='val_rmse',\n",
        "                                                                     patience=patience,\n",
        "                                                                     mode=\"min\",\n",
        "                                                                     start_from_epoch=5,\n",
        "                                                                     restore_best_weights=True),\n",
        "                                      keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                                      save_weights_only=True,\n",
        "                                                                      monitor=\"val_rmse\",\n",
        "                                                                      mode='min',\n",
        "                                                                      save_best_only=True),\n",
        "                                      keras.callbacks.ReduceLROnPlateau(monitor='val_rmse',\n",
        "                                                                        factor=0.5,\n",
        "                                                                        patience=patience_reduce,\n",
        "                                                                        min_lr=0.0001,\n",
        "                                                                        mode=\"min\")],\n",
        "                            validation_data=valid_dataset)\n",
        "\n",
        "        print(\"Model training finished\")\n",
        "\n",
        "      model.load_weights(checkpoint_filepath)\n",
        "      model.evaluate(valid_dataset, verbose=0)\n",
        "\n",
        "      if i>=0:\n",
        "        plot_training_session(history)\n",
        "\n",
        "      oof_res = model.predict(valid_dataset)\n",
        "      test_pred = model.predict(test_dataset)\n",
        "\n",
        "      print(f\"Out-of-Fold Shapes: {val_y.shape},{oof_res.shape}\")\n",
        "\n",
        "      rmse_score = root_mean_squared_error(val_y, oof_res)\n",
        "\n",
        "\n",
        "      fig, axs = plt.subplots(1,1, figsize=(10,4))\n",
        "      axs.scatter(oof_res, val_y)\n",
        "      axs.set_title(f\"Out-of-Fold RMSE Score: {round(rmse_score, 3)}%\")\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      print(f\"Out-of-Fold RMSE Score: {round(rmse_score, 3)}%\")\n",
        "\n",
        "      ##################################################################### Save the Model\n",
        "      model.save(f\"{folders_experiment}/model_{experiment_name}.keras\")\n",
        "\n",
        "      ##################################################################### Create Model Output\n",
        "      test_results_df.loc[:,i] = test_pred\n",
        "      all_rmse.append(round(rmse_score, 3))\n",
        "\n",
        "      #############\n",
        "      train_results_df.iloc[val_index,i] = oof_res.flatten()\n",
        "\n",
        "      gc.collect()\n",
        "\n",
        "    ##################################################################### Create Model Output\n",
        "  print(f\"All Valuation RMSE: {all_rmse}\")\n",
        "\n",
        "  return test_results_df, train_results_df"
      ],
      "metadata": {
        "id": "07SlowwyTEoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keras Tuner:"
      ],
      "metadata": {
        "id": "wI0zCTwudEQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_on=False\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "Nr8CAgyscjdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Select a Validation set:"
      ],
      "metadata": {
        "id": "L430zzRRyk2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  Xt, Xv = train_test_split(X_train, test_size=0.20, random_state=42, stratify=X_train['p_num'])\n",
        "\n",
        "  print(f\"Train Shape: {Xt.shape}, Val Shape: {Xv.shape}\")"
      ],
      "metadata": {
        "id": "MwP6-tQiyhnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  vocabulary = Xt[\"p_num\"].unique().tolist()\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(Xt, batch_size=512, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(Xv, batch_size=512, shuffle=False)"
      ],
      "metadata": {
        "id": "efgE7LA5xyn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('activation', [\"relu\",\"silu\",\"gelu\",\"selu\"])\n",
        "  hp.Float('dropout',0.15,0.40, step=0.025)\n",
        "#  hp.Float('noise',0.05,0.1, step=0.025)\n",
        "  hp.Float('reg',0.0001, 1.0,step=10,sampling=\"log\")\n",
        "  hp.Choice('num_dense_blocks', values=[1,2,3,4])\n",
        "  hp.Choice('units_dense', values=[128,256,512])"
      ],
      "metadata": {
        "id": "y0GrdW7W1X_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_model_v0(vocabulary=vocabulary,\n",
        "                          dense_layers=[hp.get('units_dense')]+[int(hp.get('units_dense')/2)]*hp.get('num_dense_blocks')+[int(hp.get('units_dense')/4)],\n",
        "                          dropout=hp.get('dropout'),\n",
        "                          activation=hp.get('activation'),\n",
        "                          noise=0, #hp.get('noise'),\n",
        "                          reg=hp.get('reg'),\n",
        "                          lr=0.0025)\n",
        "  return model"
      ],
      "metadata": {
        "id": "rFJpjVGB1hns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_rmse\", \"min\"),\n",
        "                              hyperparameters=hp, max_trials=50, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_rmse', patience=7, mode=\"min\", start_from_epoch=5)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3, min_lr=0.0001, mode=\"min\")\n",
        "\n",
        "  tuner.search(train_dataset, validation_data=valid_dataset, epochs=16, callbacks=[stop_early,reduce_])"
      ],
      "metadata": {
        "id": "2x6sqh_27UzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    val_rmse: 1.7364014387130737\n",
        "\n",
        "    Best val_rmse So Far: 1.736088514328003\n",
        "    Total elapsed time: 05h 27m 20s\n",
        "\n",
        "    Search: Running Trial #32\n",
        "\n",
        "    Value             |Best Value So Far |Hyperparameter\n",
        "    relu              |silu              |activation\n",
        "    0.375             |0.35              |dropout\n",
        "    0.01              |0.001             |reg\n",
        "    4                 |1                 |num_dense_blocks\n",
        "    128               |512               |units_dense\n"
      ],
      "metadata": {
        "id": "rPLbcXNg-igM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  tuner.results_summary(num_trials=10)"
      ],
      "metadata": {
        "id": "oC-pdn7J7ok4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)"
      ],
      "metadata": {
        "id": "n8X6MeDZDMZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* {'activation': 'gelu', 'dropout': 0.15, 'reg': 0.0001, 'num_dense_blocks': 1, 'units_dense': 128}\n",
        "* {'activation': 'silu', 'dropout': 0.35, 'reg': 0.001, 'num_dense_blocks': 1, 'units_dense': 512}\n",
        "* {'activation': 'relu', 'dropout': 0.375, 'reg': 0.0001, 'num_dense_blocks': 1, 'units_dense': 512}\n",
        "* {'activation': 'relu', 'dropout': 0.175, 'reg': 0.1, 'num_dense_blocks': 3, 'units_dense': 128}\n",
        "\n",
        "* Trial 47 summary\n",
        "Hyperparameters:\n",
        "activation: gelu\n",
        "dropout: 0.15\n",
        "reg: 0.0001\n",
        "num_dense_blocks: 1\n",
        "units_dense: 128\n",
        "Score: 1.727567195892334\n",
        "\n",
        "* Trial 11 summary\n",
        "Hyperparameters:\n",
        "activation: silu\n",
        "dropout: 0.35\n",
        "reg: 0.001\n",
        "num_dense_blocks: 1\n",
        "units_dense: 512\n",
        "Score: 1.736088514328003\n",
        "\n",
        "* Trial 35 summary\n",
        "Hyperparameters:\n",
        "activation: relu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "num_dense_blocks: 1\n",
        "units_dense: 512\n",
        "Score: 1.7361124753952026\n",
        "\n",
        "* Trial 13 summary\n",
        "Hyperparameters:\n",
        "activation: relu\n",
        "dropout: 0.175\n",
        "reg: 0.1\n",
        "num_dense_blocks: 3\n",
        "units_dense: 128\n",
        "Score: 1.7361220121383667\n",
        "\n",
        "* Trial 14 summary\n",
        "Hyperparameters:\n",
        "activation: gelu\n",
        "dropout: 0.275\n",
        "reg: 0.001\n",
        "num_dense_blocks: 1\n",
        "units_dense: 512\n",
        "Score: 1.7361259460449219\n",
        "\n",
        "* Trial 26 summary\n",
        "Hyperparameters:\n",
        "activation: selu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "num_dense_blocks: 4\n",
        "units_dense: 128\n",
        "Score: 1.7361326217651367"
      ],
      "metadata": {
        "id": "12Yp7qG0sZVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit The Model:"
      ],
      "metadata": {
        "id": "9EZ3DWOkad4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#del df_train_scaled #Xt, Xv, train_dataset, valid_dataset, stop_early, reduce_#, tuner\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "DFYsNJS1JV0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bas = 512\n",
        "rep=1\n",
        "[bas]+[int(bas/2)]*rep+[int(bas/4)]"
      ],
      "metadata": {
        "id": "wENu5cDm1aCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best_params = tuner.get_best_hyperparameters(1)[0].values\n",
        "best_params = {'activation': 'gelu', 'dropout': 0.15, 'reg': 0.0001, 'dense_layers': [128,64,32]}"
      ],
      "metadata": {
        "id": "irgz7SIZai2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df, train_results_df = run_experiment(X_train, X_test, create_model_v0, best_params, experiment_name = \"baseline_nn_ext_v2_st\",\n",
        "                                                  rs=42, target=\"bg+1:00\", batch_size=512, num_epochs=51, learning_rate=0.0005,\n",
        "                                                  n_splits = 3, n_repeats = 3, vocabulary = vocabulary_pnum)"
      ],
      "metadata": {
        "id": "ocfx0fVAbBWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All Valuation RMSE: [1.71, 1.701, 1.715, 1.716, 1.708, 1.702, 1.701, 1.708, 1.708]"
      ],
      "metadata": {
        "id": "2Ga0QcN4L9eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df[\"average\"] = test_results_df.mean(axis=1)\n",
        "test_results_df"
      ],
      "metadata": {
        "id": "Z1RU13t1Rcv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2,2, figsize=(10,4))\n",
        "axs = np.ravel(axs)\n",
        "\n",
        "axs[0].scatter(test_results_df[\"average\"], test_results_df[0])\n",
        "axs[0].set_title(\"Average Predictions vs. Val 0\")\n",
        "\n",
        "axs[1].scatter(test_results_df[\"average\"], test_results_df[1])\n",
        "axs[1].set_title(\"Average Predictions vs. Val 1\")\n",
        "\n",
        "axs[2].scatter(test_results_df[1], test_results_df[2])\n",
        "axs[2].set_title(\"Val 1 Predictions vs. Val 2\")\n",
        "\n",
        "axs[3].scatter(test_results_df[2], test_results_df[8])\n",
        "axs[3].set_title(\"Val 2 Predictions vs. Val 8\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "70r-Bd3KToSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **STORE RESULTS**"
      ],
      "metadata": {
        "id": "BSgIWyqwSZW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "UPNAlTPRSZW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, col in enumerate(test_results_df.columns):\n",
        "  sub[\"bg+1:00\"] = test_results_df[col].values\n",
        "  sub.to_csv(f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/submissions/Submission_val_conv_v2_{col}_expanded_st.csv\", index=False)\n",
        "  print(sub.isna().sum())"
      ],
      "metadata": {
        "id": "acj0IArhSZW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results_df.head(10)"
      ],
      "metadata": {
        "id": "skHJ_ynWS2Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results_df[\"average\"] = train_results_df.mean(axis=1)\n",
        "train_results_df.isna().sum()"
      ],
      "metadata": {
        "id": "-OAy_XO-dtvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results_df_ = train_results_df[\"average\"].astype(\"float\").copy()\n",
        "train_results_df_.columns = [\"bg+1:00_v0_expanded\"]\n",
        "train_results_df_.to_csv(f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/submissions/train_results_v2_expanded_st.csv.csv\")"
      ],
      "metadata": {
        "id": "IsiYI-cig4f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 MODEL 02**"
      ],
      "metadata": {
        "id": "e3TyNwtOLl6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Dataset extentions\n",
        "\n",
        "colx = ['steps_av0','steps_av1','steps_av2','steps_av3','steps_av4','steps_av5','steps_av6','steps_av7','steps_av8','steps_av9','steps_av10','steps_av11','steps_av12','steps_av13','steps_av14',\n",
        "        'steps_av15','steps_av16','steps_av17','steps_av18','steps_av19','steps_av20','steps_av21','steps_av22','steps_av23','steps_av24','steps_av25','steps_av26','steps_av27','steps_av28','steps_av29','steps_av30','steps_av31',\n",
        "        'insulin_av0','insulin_av1','insulin_av2','insulin_av3','insulin_av4','insulin_av5','insulin_av6','insulin_av7','insulin_av8','insulin_av9','insulin_av10','insulin_av11',\n",
        "        'insulin_av12','insulin_av13','insulin_av14','insulin_av15','insulin_av16','insulin_av17','insulin_av18','insulin_av19','insulin_av20','insulin_av21','insulin_av22','insulin_av23','insulin_av24','insulin_av25','insulin_av26',\n",
        "        'insulin_av27','insulin_av28','insulin_av29','insulin_av30','insulin_av31','activity0','activity1','activity2','activity3','activity4','activity5','activity6','activity7','activity8','activity9','activity10',\n",
        "        'activity11','activity12','activity13','activity14','activity15','activity16','activity17','activity18','activity19','activity20','activity21','activity22','activity23','activity24','activity25',\n",
        "        'activity26','activity27','activity28','activity29','activity30','activity31','cals_av0','cals_av1','cals_av2','cals_av3','cals_av4','cals_av5','cals_av6','cals_av7','cals_av8','cals_av9',\n",
        "        'cals_av10','cals_av11','cals_av12','cals_av13','cals_av14','cals_av15','cals_av16','cals_av17','cals_av18','cals_av19','cals_av20','cals_av21','cals_av22','cals_av23','cals_av24','cals_av25','cals_av26','cals_av27','cals_av28','cals_av29','cals_av30','cals_av31',\n",
        "        'brake0','brake1','brake2','brake3','brake4','brake5','brake6','brake7','brake8','brake9','brake10','brake11','brake12','brake13',\n",
        "        'brake14','brake15','brake16','brake17','brake18','brake19','brake20','brake21','brake22','brake23','brake24','brake25','brake26','brake27','brake28','brake29','brake30','brake31',\n",
        "        'carbs_av0','carbs_av1','carbs_av2','carbs_av3','carbs_av4','carbs_av5','carbs_av6','carbs_av7','carbs_av8','carbs_av9','carbs_av10','carbs_av11','carbs_av12','carbs_av13','carbs_av14','carbs_av15','carbs_av16',\n",
        "        'carbs_av17','carbs_av18','carbs_av19','carbs_av20','carbs_av21','carbs_av22','carbs_av23','carbs_av24','carbs_av25','carbs_av26','carbs_av27','carbs_av28','carbs_av29','carbs_av30','carbs_av31',\n",
        "        'bg0','bg1','bg2','bg3','bg4','bg5','bg6','bg7','bg8','bg9','bg10','bg11','bg12','bg13','bg14','bg15','bg16','bg17','bg18','bg19','bg20','bg21','bg22','bg23','bg24','bg25','bg26','bg27','bg28','bg29','bg30','bg31',\n",
        "        'hr0','hr1','hr2','hr3','hr4','hr5','hr6','hr7','hr8','hr9','hr10','hr11','hr12','hr13','hr14','hr15','hr16','hr17','hr18','hr19','hr20','hr21','hr22','hr23','hr24','hr25','hr26','hr27','hr28','hr29','hr30','hr31',\n",
        "        'intake0','intake1','intake2','intake3','intake4','intake5','intake6','intake7','intake8','intake9','intake10','intake11','intake12','intake13','intake14',\n",
        "        'intake15','intake16','intake17','intake18','intake19','intake20','intake21','intake22','intake23','intake24','intake25','intake26','intake27','intake28','intake29','intake30','intake31']\n",
        "\n",
        "usecols = ['bg0','bg1','bg2','bg3','bg4','bg5','bg6','bg7','bg8','bg9','bg10','bg11','bg12','bg13','bg14','bg15','bg16','bg17','bg18','bg19','bg20','bg21','bg22','bg23','bg24','bg25','bg26','bg27','bg28','bg29','bg30','bg31',\n",
        "           'insulin_av0','insulin_av1','insulin_av2','insulin_av3','insulin_av4','insulin_av5','insulin_av6','insulin_av7','insulin_av8','insulin_av9','insulin_av10','insulin_av11','insulin_av12','insulin_av13','insulin_av14',\n",
        "           'insulin_av15','insulin_av16','insulin_av17','insulin_av18','insulin_av19','insulin_av20','insulin_av21','insulin_av22','insulin_av23','insulin_av24','insulin_av25','insulin_av26', 'insulin_av27','insulin_av28',\n",
        "           'insulin_av29','insulin_av30','insulin_av31','brake0','brake1','brake2','brake3','brake4','brake5','brake6','brake7','brake8','brake9','brake10','brake11','brake12','brake13', 'brake14','brake15','brake16','brake17',\n",
        "           'brake18','brake19','brake20','brake21','brake22','brake23','brake24','brake25','brake26','brake27','brake28','brake29','brake30','brake31','intake0','intake1','intake2','intake3','intake4','intake5','intake6','intake7',\n",
        "           'intake8','intake9','intake10','intake11','intake12','intake13','intake14','intake15','intake16','intake17','intake18','intake19','intake20','intake21','intake22','intake23','intake24','intake25','intake26','intake27','intake28','intake29','intake30','intake31']\n",
        "\n",
        "dict_colx = {i:np.float32 for i in colx}\n",
        "\n",
        "df_train_ext=pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/X_train_scaled_smaller.csv\", dtype=dict_colx, usecols=usecols)\n",
        "df_test_ext=pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/X_test_scaled_smaller.csv\", dtype=dict_colx, usecols=usecols)\n",
        "\n",
        "df_test_ext.shape"
      ],
      "metadata": {
        "id": "L0UUPpij0Kk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_ext.index = X_test.index\n",
        "#X_test.head(3)"
      ],
      "metadata": {
        "id": "J9uJQb1zBUKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(usecols)\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "OFGqZCJC2uo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_ext.head(3)"
      ],
      "metadata": {
        "id": "J8_wwM-36HU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,df_train_ext.shape"
      ],
      "metadata": {
        "id": "ziEnJcZA5dCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_ext = StandardScaler()\n",
        "df_train_ext[usecols] = scaler_ext.fit_transform(df_train_ext[usecols])\n",
        "df_test_ext[usecols] = scaler_ext.transform(df_test_ext[usecols])"
      ],
      "metadata": {
        "id": "gOhzcbuc9aQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_ext.max().max(),df_train_ext.min().min(),df_train_ext[\"bg18\"].mean()"
      ],
      "metadata": {
        "id": "kBlfshE087s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_ext.iloc[:,-55:-45].describe()"
      ],
      "metadata": {
        "id": "uCWrgxGN64Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.concat([df_train_ext,X_train],axis=1)\n",
        "X_test = pd.concat([df_test_ext,X_test],axis=1)\n",
        "X_train.shape,X_test.shape"
      ],
      "metadata": {
        "id": "RGSWBYpY5nC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "1v4BFXnfliFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[\"diff\"] = X_train[\"bg31\"]-X_train[\"bg0\"]\n",
        "X_test[\"diff\"] = X_test[\"bg31\"]-X_test[\"bg0\"]\n",
        "\n",
        "scaler_diff = StandardScaler()\n",
        "X_train[\"diff\"] = scaler_diff.fit_transform(X_train[[\"diff\"]])\n",
        "X_test[\"diff\"] = scaler_diff.transform(X_test[[\"diff\"]])"
      ],
      "metadata": {
        "id": "wFkhDF4vsHZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[[\"diff\",\"bg31\",\"insulin_av31\",\"bg+1:00\",'PC_1', 'PC_2', 'PC_3', 'PC_4', 'enc_08_v6', 'enc_07_v7', 'enc_03_v7', 'enc_01_v5',\n",
        "              'enc_02_v5', 'enc_03_v5', 'enc_04_v5', 'enc_05_v5', 'enc_06_v5', 'enc_07_v5', 'enc_08_v5']].corr()"
      ],
      "metadata": {
        "id": "GXO-ho9zmv8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feat = \"insulin_av\"\n",
        "# for i_ in range(31):\n",
        "#   ver = X_train[[f\"{feat}{i_}\",f\"{feat}31\",\"bg+1:00\"]].copy()\n",
        "#   ver[\"diff\"] = ver[f\"{feat}31\"]-ver[f\"{feat}{i_}\"]\n",
        "#   print(\"{}{} - Corr diff: {} - Corr Level: {}\".format(feat,i_,np.round(ver[\"diff\"].corr(ver[\"bg+1:00\"]), 3),np.round(ver[f\"{feat}{i_}\"].corr(ver[\"bg+1:00\"]), 3)))"
      ],
      "metadata": {
        "id": "UVAswszwq30d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train[[\"diff\",\"bg31\",\"insulin_av31\",\"bg+1:00\",'p_num','PC_1', 'PC_2', 'PC_3', 'PC_4', 'enc_08_v6',\n",
        "                   'enc_07_v7', 'enc_03_v7', 'enc_01_v5','enc_02_v5', 'enc_03_v5', 'enc_04_v5', 'enc_05_v5',\n",
        "                   'enc_06_v5', 'enc_07_v5', 'enc_08_v5']]\n",
        "\n",
        "X_test = X_test[[\"diff\",\"bg31\",\"insulin_av31\",\"bg+1:00\",'p_num','PC_1', 'PC_2', 'PC_3', 'PC_4', 'enc_08_v6',\n",
        "                   'enc_07_v7', 'enc_03_v7', 'enc_01_v5','enc_02_v5', 'enc_03_v5', 'enc_04_v5', 'enc_05_v5',\n",
        "                   'enc_06_v5', 'enc_07_v5', 'enc_08_v5']]\n",
        "\n",
        "X_train.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/X_train_ext_new.csv\")\n",
        "X_test.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/X_test_ext_new.csv\")\n",
        "\n",
        "# X_train=pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/X_train_ext_new.csv\")\n",
        "# X_test=pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/X_test_ext_new.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "E6ZzCY9IsFVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head(3)"
      ],
      "metadata": {
        "id": "U2qb_JxmBF5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Create Dataloader:"
      ],
      "metadata": {
        "id": "4K6y80VFLl6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "IMTM3EHaLl6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_to_dataset(dataframe_, shuffle=False, batch_size=32):\n",
        "    dataframe = dataframe_.copy()\n",
        "    target = dataframe[\"bg+1:00\"]\n",
        "    dataframe = dataframe.drop(columns=[\"bg+1:00\"])\n",
        "    tab_data = dataframe.drop(columns=[\"p_num\"])\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((dataframe[\"p_num\"].values,  # First input\n",
        "                                              tab_data.values), # Second input\n",
        "                                              target))\n",
        "\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(batch_size)\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "Rlddr760Ll6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataframe_to_dataset(X_train,batch_size=1)"
      ],
      "metadata": {
        "id": "wwjy3iRnLl6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TEST THE DATALOADER:**"
      ],
      "metadata": {
        "id": "OgKL5kSDLl6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (x0, x1), y in train_ds.take(1):\n",
        "    print(x0.shape,x1.shape,y.shape)"
      ],
      "metadata": {
        "id": "p8sJgNr4Ll6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.iloc[:2000,:].to_csv(\"train_sample.csv\")"
      ],
      "metadata": {
        "id": "GBMRsyWXLl6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.3 Create Models:**"
      ],
      "metadata": {
        "id": "7nNnXaIcLl6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "x4dLR1nrLl6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_v0(vocabulary=vocabulary_pnum,\n",
        "                    dense_layers=[256,128,128,64],\n",
        "                    dropout=0.3,\n",
        "                    activation=\"selu\",\n",
        "                    reg=0.0,\n",
        "                    lr=0.001,\n",
        "                    noise=0.05,\n",
        "                    summary=False):\n",
        "\n",
        "  # --- Tabular Data Processing ---\n",
        "  # Input layer for tabular data as a dictionary\n",
        "\n",
        "  pnum_input = keras.Input(shape=(1,), name=\"pnum_input\", dtype=\"string\")\n",
        "\n",
        "  # Lookup Layer for the p_num:\n",
        "  lookup = layers.StringLookup(vocabulary=vocabulary,mask_token=None,num_oov_indices=1,\n",
        "                output_mode=\"int\", name=\"lookup_pnum\")\n",
        "  pnum_encoded = lookup(pnum_input)\n",
        "\n",
        "  # Embedding layers for hour and minute\n",
        "  pnum_encoded_embedding = layers.Embedding(input_dim=len(vocabulary)+1,\n",
        "                                            output_dim=5,\n",
        "                                            name=\"embed_pnum\")(pnum_encoded)\n",
        "  # Flatten the embeddings\n",
        "  pnum_flat = layers.Flatten()(pnum_encoded_embedding)\n",
        "\n",
        "  # Input layer for Tabular Data\n",
        "  tab_data = keras.Input(shape=(18,), name=\"tab_input\")\n",
        "\n",
        "  # Concatenate all tabular features\n",
        "  x = layers.concatenate([tab_data, pnum_flat])\n",
        "  x = layers.BatchNormalization()(x)\n",
        "\n",
        "  x = layers.GaussianNoise(stddev=noise, seed=42, name=\"gn_00\")(x)\n",
        "  tabular_output = x\n",
        "  tabular_output = layers.Dropout(dropout, name=\"bb_00\")(tabular_output)\n",
        "\n",
        "  # Dense layers for tabular data\n",
        "  for num, den in enumerate(dense_layers):\n",
        "    tabular_output = dense_block(den,dropout=dropout,activation=activation, reg=reg, name=f\"block_{num}\")(tabular_output)\n",
        "\n",
        "\n",
        "  tabular_output = layers.concatenate([x, tabular_output])\n",
        "  # --- Output Layer ---\n",
        "  # Final dense layer for prediction\n",
        "  output = layers.Dense(1, name=\"output\")(tabular_output)\n",
        "\n",
        "  # --- Create and Compile the Model ---\n",
        "  # Create the model\n",
        "  model = keras.Model(inputs=[pnum_input, tab_data], outputs=output)\n",
        "  optimizer= keras.optimizers.Adam(learning_rate=lr)\n",
        "  metric = RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
        "  metric_mae = MeanAbsoluteError(name=\"mae\", dtype=None)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[metric,metric_mae])\n",
        "\n",
        "  # Print model summary\n",
        "  if summary==True:\n",
        "    model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_model_v0(vocabulary=vocabulary_pnum,summary=True)"
      ],
      "metadata": {
        "id": "1y5qpCXILl6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True, rankdir=\"LR\", expand_nested=True, show_layer_activations=True)"
      ],
      "metadata": {
        "id": "c2b3kVuyLl6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Training Functions:"
      ],
      "metadata": {
        "id": "5anK1yauLl6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(len(val_sets)))"
      ],
      "metadata": {
        "id": "dsLAol-6Ll6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function:"
      ],
      "metadata": {
        "id": "WOIp0LNbLl6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(X_train, X_test, model_constructor, best_params, experiment_name=\"baseline_nn_ext_v1\", rs=42, target=\"bg+1:00\",\n",
        "                   batch_size=64, num_epochs=200, learning_rate=0.001, n_splits = 5, n_repeats = 5, vocabulary=vocabulary_pnum,\n",
        "                   patience=11,patience_reduce=5):\n",
        "\n",
        "  test_predictions = np.zeros((len(X_test),1))\n",
        "  test_results_df = pd.DataFrame(index=X_test.index, columns=list(range(n_repeats*n_splits)))\n",
        "  train_results_df = pd.DataFrame(index=X_train.index, columns=list(range(n_repeats*n_splits)))\n",
        "\n",
        "  all_mse = []\n",
        "  all_rmse = []\n",
        "\n",
        "  rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=36851234)\n",
        "\n",
        "  for i, (train_index, val_index) in enumerate(rskf.split(X_train, X_train[\"p_num\"])):\n",
        "      print(f\"\\nRunning CV {i}\\n\")\n",
        "      X, val_X = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "      y, val_y = X_train[target].iloc[train_index], X_train[target].iloc[val_index]\n",
        "      X_test = X_test.copy()\n",
        "      #################################################################### Prepare Datasets loaders:\n",
        "      train_dataset = dataframe_to_dataset(X, batch_size=batch_size, shuffle=True)\n",
        "      valid_dataset = dataframe_to_dataset(val_X, batch_size=batch_size, shuffle=False)\n",
        "      test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "      ##################################################################### Relevant Folders\n",
        "      folders_experiment = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/Glucose/{experiment_name}_{i}/\"\n",
        "      ##################################################################### Generate and Fit Model\n",
        "      # Callbacks:\n",
        "      checkpoint_filepath = folders_experiment + f'checkpoint/{experiment_name}_{i}.weights.h5'\n",
        "\n",
        "      # Generate the Model:\n",
        "      model = model_constructor(vocabulary=vocabulary,\n",
        "                                lr = learning_rate,\n",
        "                                **best_params)\n",
        "      if i>=0:\n",
        "        print(\"Start training the model...\")\n",
        "\n",
        "        history = model.fit(train_dataset,\n",
        "                            epochs=num_epochs,\n",
        "                            callbacks=[keras.callbacks.EarlyStopping(monitor='val_rmse',\n",
        "                                                                     patience=patience,\n",
        "                                                                     mode=\"min\",\n",
        "                                                                     start_from_epoch=5,\n",
        "                                                                     restore_best_weights=True),\n",
        "                                      keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                                      save_weights_only=True,\n",
        "                                                                      monitor=\"val_rmse\",\n",
        "                                                                      mode='min',\n",
        "                                                                      save_best_only=True),\n",
        "                                      keras.callbacks.ReduceLROnPlateau(monitor='val_rmse',\n",
        "                                                                        factor=0.5,\n",
        "                                                                        patience=patience_reduce,\n",
        "                                                                        min_lr=0.0001,\n",
        "                                                                        mode=\"min\")],\n",
        "                            validation_data=valid_dataset)\n",
        "\n",
        "        print(\"Model training finished\")\n",
        "\n",
        "      model.load_weights(checkpoint_filepath)\n",
        "      model.evaluate(valid_dataset, verbose=0)\n",
        "\n",
        "      if i>=0:\n",
        "        plot_training_session(history)\n",
        "\n",
        "      oof_res = model.predict(valid_dataset)\n",
        "      test_pred = model.predict(test_dataset)\n",
        "\n",
        "      print(f\"Out-of-Fold Shapes: {val_y.shape},{oof_res.shape}\")\n",
        "\n",
        "      rmse_score = root_mean_squared_error(val_y, oof_res)\n",
        "\n",
        "\n",
        "      fig, axs = plt.subplots(1,1, figsize=(10,4))\n",
        "      axs.scatter(oof_res, val_y)\n",
        "      axs.set_title(f\"Out-of-Fold RMSE Score: {round(rmse_score, 3)}%\")\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      print(f\"Out-of-Fold RMSE Score: {round(rmse_score, 3)}%\")\n",
        "\n",
        "      ##################################################################### Save the Model\n",
        "      model.save(f\"{folders_experiment}/model_{experiment_name}.keras\")\n",
        "\n",
        "      ##################################################################### Create Model Output\n",
        "      test_results_df.loc[:,i] = test_pred\n",
        "      all_rmse.append(round(rmse_score, 3))\n",
        "\n",
        "      #############\n",
        "      train_results_df.iloc[val_index,i] = oof_res.flatten()\n",
        "\n",
        "      gc.collect()\n",
        "\n",
        "    ##################################################################### Create Model Output\n",
        "  print(f\"All Valuation RMSE: {all_rmse}\")\n",
        "\n",
        "  return test_results_df, train_results_df"
      ],
      "metadata": {
        "id": "0kAttfTyLl6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keras Tuner:"
      ],
      "metadata": {
        "id": "tChN5TSXFBiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_on=False\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "TLrD-hmcFBiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Select a Validation set:"
      ],
      "metadata": {
        "id": "kdt7440kFBiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  Xt, Xv = train_test_split(X_train, test_size=0.20, random_state=42, stratify=X_train['p_num'])\n",
        "\n",
        "  print(f\"Train Shape: {Xt.shape}, Val Shape: {Xv.shape}\")"
      ],
      "metadata": {
        "id": "Pvo0rKK0FBiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  vocabulary = Xt[\"p_num\"].unique().tolist()\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(Xt, batch_size=512, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(Xv, batch_size=512, shuffle=False)"
      ],
      "metadata": {
        "id": "AjYcOJvxFBiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('activation', [\"relu\",\"silu\",\"gelu\",\"selu\"])\n",
        "  hp.Float('dropout',0.15,0.40, step=0.025)\n",
        "#  hp.Float('noise',0.05,0.1, step=0.025)\n",
        "  hp.Float('reg',0.0001, 1.0,step=10,sampling=\"log\")\n",
        "  hp.Choice('num_dense_blocks', values=[1,2,3,4])\n",
        "  hp.Choice('units_dense', values=[128,256,512])"
      ],
      "metadata": {
        "id": "AGIPXtAdFBiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_model_v0(vocabulary=vocabulary,\n",
        "                          dense_layers=[hp.get('units_dense')]+[int(hp.get('units_dense')/2)]*hp.get('num_dense_blocks')+[int(hp.get('units_dense')/4)],\n",
        "                          dropout=hp.get('dropout'),\n",
        "                          activation=hp.get('activation'),\n",
        "                          noise=0, #hp.get('noise'),\n",
        "                          reg=hp.get('reg'),\n",
        "                          lr=0.0025)\n",
        "  return model"
      ],
      "metadata": {
        "id": "ELRleSd4FBiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_rmse\", \"min\"),\n",
        "                              hyperparameters=hp, max_trials=50, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_rmse', patience=7, mode=\"min\", start_from_epoch=5)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3, min_lr=0.0001, mode=\"min\")\n",
        "\n",
        "  tuner.search(train_dataset, validation_data=valid_dataset, epochs=16, callbacks=[stop_early,reduce_])"
      ],
      "metadata": {
        "id": "Vv6KJ9PIFBiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    val_rmse: 1.7364014387130737\n",
        "\n",
        "    Best val_rmse So Far: 1.736088514328003\n",
        "    Total elapsed time: 05h 27m 20s\n",
        "\n",
        "    Search: Running Trial #32\n",
        "\n",
        "    Value             |Best Value So Far |Hyperparameter\n",
        "    relu              |silu              |activation\n",
        "    0.375             |0.35              |dropout\n",
        "    0.01              |0.001             |reg\n",
        "    4                 |1                 |num_dense_blocks\n",
        "    128               |512               |units_dense\n"
      ],
      "metadata": {
        "id": "RcXxYly_FBiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  tuner.results_summary(num_trials=10)"
      ],
      "metadata": {
        "id": "cqzYwtSPFBiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)"
      ],
      "metadata": {
        "id": "AgbxBiCTFBiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* {'activation': 'gelu', 'dropout': 0.15, 'reg': 0.0001, 'num_dense_blocks': 1, 'units_dense': 128}\n",
        "* {'activation': 'silu', 'dropout': 0.35, 'reg': 0.001, 'num_dense_blocks': 1, 'units_dense': 512}\n",
        "* {'activation': 'relu', 'dropout': 0.375, 'reg': 0.0001, 'num_dense_blocks': 1, 'units_dense': 512}\n",
        "* {'activation': 'relu', 'dropout': 0.175, 'reg': 0.1, 'num_dense_blocks': 3, 'units_dense': 128}\n",
        "\n",
        "* Trial 47 summary\n",
        "Hyperparameters:\n",
        "activation: gelu\n",
        "dropout: 0.15\n",
        "reg: 0.0001\n",
        "num_dense_blocks: 1\n",
        "units_dense: 128\n",
        "Score: 1.727567195892334\n",
        "\n",
        "* Trial 11 summary\n",
        "Hyperparameters:\n",
        "activation: silu\n",
        "dropout: 0.35\n",
        "reg: 0.001\n",
        "num_dense_blocks: 1\n",
        "units_dense: 512\n",
        "Score: 1.736088514328003\n",
        "\n",
        "* Trial 35 summary\n",
        "Hyperparameters:\n",
        "activation: relu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "num_dense_blocks: 1\n",
        "units_dense: 512\n",
        "Score: 1.7361124753952026\n",
        "\n",
        "* Trial 13 summary\n",
        "Hyperparameters:\n",
        "activation: relu\n",
        "dropout: 0.175\n",
        "reg: 0.1\n",
        "num_dense_blocks: 3\n",
        "units_dense: 128\n",
        "Score: 1.7361220121383667\n",
        "\n",
        "* Trial 14 summary\n",
        "Hyperparameters:\n",
        "activation: gelu\n",
        "dropout: 0.275\n",
        "reg: 0.001\n",
        "num_dense_blocks: 1\n",
        "units_dense: 512\n",
        "Score: 1.7361259460449219\n",
        "\n",
        "* Trial 26 summary\n",
        "Hyperparameters:\n",
        "activation: selu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "num_dense_blocks: 4\n",
        "units_dense: 128\n",
        "Score: 1.7361326217651367"
      ],
      "metadata": {
        "id": "waCvEr2VFBiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit The Model:"
      ],
      "metadata": {
        "id": "rQIce-I0FOYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#del df_train_scaled #Xt, Xv, train_dataset, valid_dataset, stop_early, reduce_#, tuner\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "BVLAdMW9FOYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bas = 512\n",
        "rep=1\n",
        "[bas]+[int(bas/2)]*rep+[int(bas/4)]"
      ],
      "metadata": {
        "id": "pRSffhaDFOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best_params = tuner.get_best_hyperparameters(1)[0].values\n",
        "best_params = {'activation': 'gelu', 'dropout': 0.15, 'reg': 0.0001, 'dense_layers': [128,64,32]}"
      ],
      "metadata": {
        "id": "XNH674aEFOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df, train_results_df = run_experiment(X_train, X_test, create_model_v0, best_params, experiment_name = \"baseline_nn_extra_v3_st\",\n",
        "                                                  rs=42, target=\"bg+1:00\", batch_size=512, num_epochs=51, learning_rate=0.0005,\n",
        "                                                  n_splits = 3, n_repeats = 3, vocabulary = vocabulary_pnum)"
      ],
      "metadata": {
        "id": "6-02_FSMFOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All Valuation RMSE: [2.195, 1.99, 2.329, 0.841]"
      ],
      "metadata": {
        "id": "YaC4u6BiFOYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df[\"average\"] = test_results_df.mean(axis=1)\n",
        "test_results_df"
      ],
      "metadata": {
        "id": "tgxV4IhHFOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2,2, figsize=(10,4))\n",
        "axs = np.ravel(axs)\n",
        "\n",
        "axs[0].scatter(test_results_df[\"average\"], test_results_df[0])\n",
        "axs[0].set_title(\"Average Predictions vs. Val 0\")\n",
        "\n",
        "axs[1].scatter(test_results_df[\"average\"], test_results_df[1])\n",
        "axs[1].set_title(\"Average Predictions vs. Val 1\")\n",
        "\n",
        "axs[2].scatter(test_results_df[1], test_results_df[2])\n",
        "axs[2].set_title(\"Val 1 Predictions vs. Val 2\")\n",
        "\n",
        "axs[3].scatter(test_results_df[2], test_results_df[8])\n",
        "axs[3].set_title(\"Val 2 Predictions vs. Val 8\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6vsC80tFOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **STORE RESULTS**"
      ],
      "metadata": {
        "id": "6KH4i7hEFOYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "3ynVTWjEFOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, col in enumerate(test_results_df.columns):\n",
        "  sub[\"bg+1:00\"] = test_results_df[col].values\n",
        "  sub.to_csv(f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/submissions/Submission_val_conv_v3_{col}_expanded_st.csv\", index=False)\n",
        "  print(sub.isna().sum())"
      ],
      "metadata": {
        "id": "aHnqe8eLFOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results_df.head(10)"
      ],
      "metadata": {
        "id": "U5TcVA0qFOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results_df[\"average\"] = train_results_df.mean(axis=1)\n",
        "train_results_df.isna().sum()"
      ],
      "metadata": {
        "id": "XEUhdGGjFOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results_df_ = train_results_df[\"average\"].astype(\"float\").copy()\n",
        "train_results_df_.columns = [\"bg+1:00_v0_expanded\"]\n",
        "train_results_df_.to_csv(f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/submissions/train_results_v3_expanded_st.csv.csv\")"
      ],
      "metadata": {
        "id": "wvIYG353FOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 MODEL 03**"
      ],
      "metadata": {
        "id": "rDv0OD2P9rrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Create Dataloader:"
      ],
      "metadata": {
        "id": "uoglh43l9rrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "KZ8MNcxc9rrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_to_dataset(dataframe, shuffle=False, batch_size=32, flds = ts_fields):\n",
        "    dataframe = dataframe.copy()\n",
        "    target = dataframe[\"bg+1:00\"]\n",
        "    dataframe = dataframe.drop(columns=[\"bg+1:00\"])\n",
        "\n",
        "    timeseries_df = dataframe[flds].values\n",
        "    timeseries_df = timeseries_df.reshape((-1, 72, 8))\n",
        "    static_df = dataframe.drop(columns=flds)\n",
        "\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((static_df[\"hour\"].values,  # First input\n",
        "                                              static_df[\"minute\"].values,  # Second input\n",
        "                                              static_df[\"cluster\"].values,  # Third input\n",
        "                                              static_df[\"cluster_pca\"].values,  # Fourth input\n",
        "                                              static_df[[\"PC_1\",\"PC_2\",\"PC_3\",\"enc_04_v7\",\"enc_01_v1\",\"enc_07_v7\",\"enc_01_v4\",\"enc_05_v4\",\n",
        "                                                        \"enc_03_v6\",\"enc_05_v7\",\"enc_06_v6\",\"enc_01_v6\",\"enc_06_v7\",\"enc_08_v6\",\"enc_04_v6\"]].values,  # Fourth input\n",
        "                                              timeseries_df),\n",
        "                                              target))\n",
        "\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(batch_size)\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "MlctSTEq9rrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataframe_to_dataset(X_train,batch_size=1)"
      ],
      "metadata": {
        "id": "aQ-NJv089rrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TEST THE DATALOADER:**"
      ],
      "metadata": {
        "id": "uDGwJg4P9rrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (x0, x1, x2, x3, x4, x5), y in train_ds.take(1):\n",
        "    print(x0.shape,x1.shape,x2.shape,x3.shape,x4.shape,x5.shape,y.shape)"
      ],
      "metadata": {
        "id": "lOR2cwXC9rrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.iloc[:2000,:].to_csv(\"train_sample.csv\")"
      ],
      "metadata": {
        "id": "ec9jxtoP9rrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "OZEe1sDs9rra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.3 Create Models:**"
      ],
      "metadata": {
        "id": "rclVKLhr9rra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(input_tensor, filters, kernel_size, strides=1):\n",
        "    x = layers.Conv1D(filters, kernel_size, strides=strides, padding='same')(input_tensor)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.SpatialDropout1D(0.2)(x)\n",
        "\n",
        "    # Residual connection\n",
        "    if strides > 1 or input_tensor.shape[-1] != filters:\n",
        "        residual = layers.Conv1D(filters, 1, strides=strides, padding='same')(input_tensor)\n",
        "    else:\n",
        "        residual = input_tensor\n",
        "\n",
        "    x = layers.add([x, residual])\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "tmxm3n2H9rrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_v2(dense_layers=[256,128,128,128,64],\n",
        "                    dropout=0.3, conv_layers=[1024,512,256,128],\n",
        "                    activation=\"selu\", reg=0.0,lr=0.001,strides=1,kernel_size=3, summary=False):\n",
        "\n",
        "  # --- Tabular Data Processing ---\n",
        "  # Input layer for tabular data as a dictionary\n",
        "  hour_input = keras.Input(shape=(1,), name=\"hour_input\")\n",
        "  minute_input = keras.Input(shape=(1,), name=\"minute_input\")\n",
        "  cluster_input = keras.Input(shape=(1,), name=\"cluster_input\")\n",
        "  pca_input = keras.Input(shape=(1,), name=\"cluster_pca\")\n",
        "  cont_input = keras.Input(shape=(15,), name=\"cont_inputs\")\n",
        "\n",
        "  # Lookup Layer for the p_num:\n",
        "  # pca_lookup = layers.IntegerLookup(vocabulary=list(range(0,4)),mask_token=None,num_oov_indices=0,\n",
        "  #                                  output_mode=\"int\", name=\"lookup_pca\")\n",
        "  # pca_encoded = pca_lookup(pca_input)\n",
        "\n",
        "  # Lookup Layer for the minutes:\n",
        "  minute_lookup = layers.IntegerLookup(vocabulary=list(range(0,60,5)),mask_token=None,num_oov_indices=0,\n",
        "                                       output_mode=\"int\", name=\"lookup_min\")\n",
        "\n",
        "  minute_encoded = minute_lookup(minute_input)\n",
        "\n",
        "  # Embedding layers for hour and minute\n",
        "  hour_embedding = layers.Embedding(input_dim=24, output_dim=8, name=\"embed_hour\")(hour_input)\n",
        "  minute_embedding = layers.Embedding(input_dim=12, output_dim=6, name=\"embed_minute\")(minute_encoded)\n",
        "  pca_encoded_embedding = layers.Embedding(input_dim=4,output_dim=3, name=\"embed_pca\")(pca_input)\n",
        "\n",
        "  # Embedding layer for cluster\n",
        "  cluster_embedding = layers.Embedding(input_dim=22, output_dim=8, name=\"embed_cluster\")(cluster_input)\n",
        "\n",
        "  # Flatten the embeddings\n",
        "  hour_flat = layers.Flatten()(hour_embedding)\n",
        "  minute_flat = layers.Flatten()(minute_embedding)\n",
        "  cluster_flat = layers.Flatten()(cluster_embedding)\n",
        "  pca_flat = layers.Flatten()(pca_encoded_embedding)\n",
        "\n",
        "  # Concatenate all tabular features\n",
        "  tabular_output = layers.concatenate([hour_flat, minute_flat, cluster_flat, pca_flat,cont_input],\n",
        "                                      name=\"tabular_concat\")\n",
        "\n",
        "  # Dense layers for tabular data\n",
        "  for num, den in enumerate(dense_layers):\n",
        "    tabular_output = residual_block_dense(tabular_output, den, activation=activation, normalization_type='batch', dropout_rate=dropout, name=f\"block_{num}\",  reg=reg,)\n",
        "\n",
        "  # --- Time Series Data Processing ---\n",
        "  # Input layer for time series data\n",
        "  time_series_input = keras.Input(shape=(72, 8), name=\"time_series_input\")\n",
        "  x=time_series_input\n",
        "  # LSTM layers for time series data\n",
        "  for sup, i in enumerate(conv_layers):\n",
        "      input_tensor = x\n",
        "      x = layers.Conv1D(filters=i, kernel_size=3, strides=strides, padding='same',name=f\"conv_{sup}\")(x)\n",
        "      x = layers.LayerNormalization(name=f\"lnorm_{sup}\")(x)\n",
        "      x = layers.Activation('relu',name=f\"act_{sup}\")(x)\n",
        "      x = layers.SpatialDropout1D(0.2,name=f\"spat_drop_{sup}\")(x)\n",
        "\n",
        "      # Residual connection\n",
        "      if strides > 1 or time_series_input.shape[-1] != i:\n",
        "          residual = layers.Conv1D(i, 1, strides=strides, padding='same',name=f\"res_conn_{sup}\")(input_tensor)\n",
        "      else:\n",
        "          residual = input_tensor\n",
        "\n",
        "      x = layers.add([x, residual],name=f\"add_{sup}\")\n",
        "      x = layers.MaxPooling1D(pool_size=2,name=f\"max_pool_{sup}\")(x)\n",
        "\n",
        "  x = layers.Flatten(name=f\"flatten_conv\")(x)\n",
        "  # --- Combine Tabular and Time Series Data ---\n",
        "  # Concatenate the outputs from both branches\n",
        "  concatenated = layers.concatenate([tabular_output, x])\n",
        "\n",
        "  # --- Output Layer ---\n",
        "  # Final dense layer for prediction\n",
        "  output = layers.Dense(1,\n",
        "                        name=\"output\",\n",
        "                        #activity_regularizer=keras.regularizers.l2(reg)\n",
        "                        )(concatenated)\n",
        "\n",
        "  # --- Create and Compile the Model ---\n",
        "  # Create the model\n",
        "  model = keras.Model(inputs=[hour_input, minute_input, cluster_input, pca_input,cont_input,time_series_input], outputs=output)\n",
        "\n",
        "  optimizer= keras.optimizers.Adam(learning_rate=lr)\n",
        "  metric = RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
        "  metric_mae = MeanAbsoluteError(name=\"mae\", dtype=None)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[metric,metric_mae])\n",
        "\n",
        "  # Print model summary\n",
        "  if summary==True:\n",
        "    model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "#vocabulary = X_train[\"p_num\"].unique().tolist()\n",
        "model = create_model_v2(summary=True)"
      ],
      "metadata": {
        "id": "PF0Tkogd9rrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True, rankdir=\"LR\", expand_nested=True, show_layer_activations=True)"
      ],
      "metadata": {
        "id": "0iSf1EzM9rrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Training Functions:"
      ],
      "metadata": {
        "id": "0435jpRy9rrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(len(val_sets)))"
      ],
      "metadata": {
        "id": "RWbeUm719rrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function:"
      ],
      "metadata": {
        "id": "OOZsBzXL9rrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(X_train, X_test, model_constructor, best_params, split=val_sets, experiment_name=\"conv_v0_nn\", rs=42, target=\"bg+1:00\",\n",
        "                   batch_size=64, num_epochs=200, learning_rate=0.001, target_scaler=target_scaler):\n",
        "\n",
        "  test_predictions = np.zeros((len(X_test),1))\n",
        "  test_results_df = pd.DataFrame(index=X_test.index, columns=list(range(len(val_sets))))\n",
        "\n",
        "  all_mse = []\n",
        "  all_rmse = []\n",
        "\n",
        "  for i, val_index in enumerate(split):\n",
        "\n",
        "    print(f\"\\nRunning CV {i}\\n\")\n",
        "    ########################################################################## Prepare the Dataset:\n",
        "    X_trn = X_train.drop(index=val_index)\n",
        "    X_val = X_train.loc[val_index,:]\n",
        "\n",
        "#    vocabulary = X_trn[\"p_num\"].unique().tolist()\n",
        "\n",
        "    X = X_trn.drop(columns=[target]).copy()\n",
        "    y = X_trn[target].copy()\n",
        "\n",
        "    val_X = X_val.drop(columns=[target]).copy()\n",
        "    val_y = X_val[target].copy()\n",
        "\n",
        "    X_test = X_test.copy()\n",
        "    #################################################################### Prepare Datasets loaders:\n",
        "\n",
        "    train_dataset = dataframe_to_dataset(X_trn, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataset = dataframe_to_dataset(X_val, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    ##################################################################### Relevant Folders\n",
        "    folders_experiment = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/Glucose/{experiment_name}_{i}/\"\n",
        "    ##################################################################### Generate and Fit Model\n",
        "    # Callbacks:\n",
        "    checkpoint_filepath = folders_experiment + f'checkpoint/{experiment_name}.weights.h5'\n",
        "\n",
        "    # Generate the Model:\n",
        "    model = model_constructor(lr = learning_rate,\n",
        "                              **best_params)\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs=num_epochs,\n",
        "                        callbacks=[keras.callbacks.EarlyStopping(monitor='val_rmse', patience=17, mode=\"min\",\n",
        "                                                  start_from_epoch=5,restore_best_weights=True),\n",
        "                                   keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                    save_weights_only=True,\n",
        "                                                    monitor=\"val_rmse\",\n",
        "                                                    mode='min',\n",
        "                                                    save_best_only=True),\n",
        "                                   keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5,\n",
        "                                                          patience=5, min_lr=0.0001, mode=\"min\")],\n",
        "                        validation_data=valid_dataset)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    model.evaluate(valid_dataset, verbose=0)\n",
        "\n",
        "    plot_training_session(history)\n",
        "\n",
        "    oof_res = model.predict(valid_dataset)\n",
        "    test_pred = model.predict(test_dataset)\n",
        "\n",
        "    print(f\"Out-of-Fold Shapes: {val_y.shape},{oof_res.shape}\")\n",
        "\n",
        "    rmse_score = root_mean_squared_error(val_y, oof_res)\n",
        "\n",
        "    val_pred = target_scaler.inverse_transform(oof_res)\n",
        "    real_val_y = target_scaler.inverse_transform(val_y.values.reshape(-1,1))\n",
        "    test_pred_real = target_scaler.inverse_transform(test_pred)\n",
        "\n",
        "    rmse_score_original = root_mean_squared_error(real_val_y, val_pred)\n",
        "\n",
        "    fig, axs = plt.subplots(1,1, figsize=(10,4))\n",
        "    axs.scatter(val_pred, real_val_y)\n",
        "    axs.set_title(f\"Out-of-Fold RMSE Score: {round(rmse_score, 3)}%\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Out-of-Fold RMSE Score Rebased: {round(rmse_score, 3)}%\")\n",
        "    print(f\"Out-of-Fold RMSE Score Original: {round(rmse_score_original, 3)}%\")\n",
        "\n",
        "    ##################################################################### Save the Model\n",
        "    model.save(f\"{folders_experiment}/model_{experiment_name}.keras\")\n",
        "\n",
        "    ##################################################################### Create Model Output\n",
        "    test_results_df.loc[:,i] = test_pred_real\n",
        "    all_rmse.append(round(rmse_score_original, 3))\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "  ##################################################################### Create Model Output\n",
        "  print(f\"All Valuation RMSE: {all_rmse}\")\n",
        "\n",
        "  return test_results_df"
      ],
      "metadata": {
        "id": "Vf73pOoN9rrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keras Tuner:"
      ],
      "metadata": {
        "id": "WLDlNf_S9rrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_on=True\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "Eoe4yvU-9rrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Select a Validation set:"
      ],
      "metadata": {
        "id": "Vcoa0d9S9rrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_index = val_sets[0]\n",
        "Xt = X_train.drop(index=valid_index)\n",
        "Xv = X_train.loc[valid_index,:]\n",
        "\n",
        "print(f\"Train Shape: {Xt.shape}, Val Shape: {Xv.shape}\")"
      ],
      "metadata": {
        "id": "hf3XYhZp9rrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(Xt, batch_size=256, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(Xv, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "mqDY4VG39rrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "512/4"
      ],
      "metadata": {
        "id": "67oSN8xo9rrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('main_activation', [\"relu\",\"silu\",\"gelu\",\"selu\"]) #\n",
        "  hp.Float('dropout',0.15,0.40, step=0.025) #\n",
        "  hp.Float('reg',0.0001, 1.0,step=10,sampling=\"log\") #\n",
        "  hp.Choice('dense_layers', values=[512,256,128])\n",
        "  hp.Choice('conv_layers', values=[1024,512,256,128])"
      ],
      "metadata": {
        "id": "yQ9OozxR9rre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_model_v2(dense_layers=[hp.get('dense_layers'),int(hp.get('dense_layers')/2),int(hp.get('dense_layers')/2),int(hp.get('dense_layers')/2),int(hp.get('dense_layers')/4)],\n",
        "                          dropout=hp.get('dropout'),\n",
        "                          conv_layers=[hp.get('conv_layers'),int(hp.get('conv_layers')/2),int(hp.get('conv_layers')/4),int(hp.get('conv_layers')/8)],\n",
        "                          activation=hp.get('main_activation'),\n",
        "                          reg=hp.get('reg'),\n",
        "                          lr=0.001,\n",
        "                          summary=False)\n",
        "  return model"
      ],
      "metadata": {
        "id": "a3YWHNID9rre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_rmse\", \"min\"),\n",
        "                              hyperparameters=hp, max_trials=50, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_rmse', patience=7, mode=\"min\", start_from_epoch=5)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3, min_lr=0.0001, mode=\"min\")\n",
        "\n",
        "  tuner.search(train_dataset, validation_data=valid_dataset, epochs=31, callbacks=[stop_early,reduce_])"
      ],
      "metadata": {
        "id": "qc-5oUU59rre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  tuner.results_summary(num_trials=21)"
      ],
      "metadata": {
        "id": "Kxu7KqWY9rre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)\n",
        "  print(tuner.get_best_hyperparameters(5)[4].values)\n",
        "  print(tuner.get_best_hyperparameters(6)[5].values)\n",
        "  print(tuner.get_best_hyperparameters(7)[6].values)"
      ],
      "metadata": {
        "id": "_srg9spf9rre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* {'main_activation': 'gelu', 'dropout': 0.375, 'reg': 0.0001, 'dense_layers': 128, 'conv_layers': 128}\n",
        "* {'main_activation': 'gelu', 'dropout': 0.15, 'reg': 0.0001, 'dense_layers': 128, 'conv_layers': 128}\n",
        "* {'main_activation': 'silu', 'dropout': 0.35, 'reg': 0.0001, 'dense_layers': 128, 'conv_layers': 128}\n",
        "* {'main_activation': 'relu', 'dropout': 0.375, 'reg': 0.0001, 'dense_layers': 128, 'conv_layers': 128}\n",
        "* {'main_activation': 'relu', 'dropout': 0.275, 'reg': 0.0001, 'dense_layers': 128, 'conv_layers': 128}\n",
        "* {'main_activation': 'relu', 'dropout': 0.375, 'reg': 0.0001, 'dense_layers': 128, 'conv_layers': 128}\n",
        "* {'main_activation': 'gelu', 'dropout': 0.30, 'reg': 0.0001, 'dense_layers': 128, 'conv_layers': 128}\n",
        "\n",
        "* **Trial 34 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.9520071744918823\n",
        "\n",
        "* **Trial 30 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.15\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.9539700746536255\n",
        "\n",
        "* **Trial 22 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.35\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.954962134361267\n",
        "\n",
        "* **Trial 24 summary**\n",
        "Hyperparameters:\n",
        "main_activation: relu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.9550836086273193\n",
        "\n",
        "* **Trial 11 summary**\n",
        "Hyperparameters:\n",
        "main_activation: relu\n",
        "dropout: 0.275\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.9551788568496704\n",
        "\n",
        "* **Trial 40 summary**\n",
        "Hyperparameters:\n",
        "main_activation: relu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.955820918083191\n",
        "\n",
        "* **Trial 37 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.30000000000000004\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.9563628435134888\n",
        "\n",
        "* **Trial 26 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "dense_layers: 512\n",
        "conv_layers: 128\n",
        "Score: 1.956717610359192\n",
        "\n",
        "* **Trial 15 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.275\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.9571396112442017\n",
        "\n",
        "* **Trial 25 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.325\n",
        "reg: 0.0001\n",
        "dense_layers: 256\n",
        "conv_layers: 128\n",
        "Score: 1.9579848051071167\n",
        "\n",
        "* **Trial 44 summary**\n",
        "Hyperparameters:\n",
        "main_activation: relu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.958244800567627\n",
        "\n",
        "* **Trial 19 summary**\n",
        "Hyperparameters:\n",
        "main_activation: relu\n",
        "dropout: 0.30000000000000004\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.9587109088897705\n",
        "\n",
        "* **Trial 48 summary**\n",
        "Hyperparameters:\n",
        "main_activation: relu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 128\n",
        "Score: 1.9599860906600952\n",
        "\n",
        "* **Trial 23 summary**\n",
        "Hyperparameters:\n",
        "main_activation: relu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "dense_layers: 256\n",
        "conv_layers: 128\n",
        "Score: 1.9600061178207397"
      ],
      "metadata": {
        "id": "JAAziUcp9rrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit The Model:"
      ],
      "metadata": {
        "id": "PGrGnDtb9rrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#best_params = tuner.get_best_hyperparameters(1)[0].values\n",
        "best_params = {'activation': 'silu', 'dropout': 0.35, 'reg': 0.0001, 'dense_layers': [128,64,64,64,32], 'conv_layers': [128,64,32,16]}"
      ],
      "metadata": {
        "id": "QN5iBLBc9rrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df = run_experiment(X_train, X_test, create_model_v2, best_params, split=val_sets,\n",
        "                                 experiment_name=\"conv_v2_nn\", rs=42, target=\"bg+1:00\",\n",
        "                                 batch_size=256, num_epochs=200, learning_rate=0.00025)"
      ],
      "metadata": {
        "id": "jR7CUwIc9rrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df.clip(2.7,22.5, inplace=True)"
      ],
      "metadata": {
        "id": "vPK3kQzTIq9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(test_results_df[0],test_results_df[4])"
      ],
      "metadata": {
        "id": "s9I-glmH60Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df[\"average\"] = test_results_df.mean(axis=1)\n",
        "test_results_df.max(axis=0),test_results_df.min(axis=0)"
      ],
      "metadata": {
        "id": "IODvqfJA9rrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **STORE RESULTS**"
      ],
      "metadata": {
        "id": "VIfZv74V9rrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "bE8YK5Mk9rrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, col in enumerate(test_results_df.columns):\n",
        "  sub[\"bg+1:00\"] = test_results_df[col].values\n",
        "  sub.to_csv(f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/submissions/Submission_val_conv_v3_{col}_clipped.csv\", index=False)\n",
        "  print(sub.isna().sum())"
      ],
      "metadata": {
        "id": "wTRfkHld9rrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4 MODEL 04**"
      ],
      "metadata": {
        "id": "_3U8xN0bJ5ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Create Dataloader:"
      ],
      "metadata": {
        "id": "5OzGGjClJ5ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[[\"PC_1\",\"PC_2\",\"PC_3\",\"enc_04_v7\",\"enc_01_v1\",\"enc_07_v7\",\"enc_01_v4\",\n",
        "          \"enc_05_v4\",\"enc_03_v6\",\"enc_05_v7\",\"enc_06_v6\",\"enc_01_v6\",\"enc_06_v7\",\n",
        "          \"enc_08_v6\",\"enc_04_v6\",\"bg-0:00\",\"insulin_av-0:00\",\"brake-0:00\",\"intake-0:00\"]].describe()"
      ],
      "metadata": {
        "id": "hp1yj3hrZGPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[[\"PC_1\",\"PC_2\",\"PC_3\",\"enc_04_v7\",\"enc_01_v1\",\"enc_07_v7\",\"enc_01_v4\",\n",
        "          \"enc_05_v4\",\"enc_03_v6\",\"enc_05_v7\",\"enc_06_v6\",\"enc_01_v6\",\"enc_06_v7\",\n",
        "          \"enc_08_v6\",\"enc_04_v6\",\"bg-0:00\",\"insulin_av-0:00\",\"brake-0:00\",\"intake-0:00\"]].describe()"
      ],
      "metadata": {
        "id": "laCSiFdgZStU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "8eq7gXULJ5mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_to_dataset(dataframe, shuffle=False, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    target = dataframe[\"bg+1:00\"]\n",
        "    dataframe = dataframe.drop(columns=[\"bg+1:00\"])\n",
        "\n",
        "    static_df = dataframe.copy()\n",
        "\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((static_df[\"hour\"].values,  # First input\n",
        "                                              static_df[\"minute\"].values,  # Second input\n",
        "                                              static_df[\"cluster\"].values,  # Third input\n",
        "                                              static_df[\"cluster_pca\"].values,  # Fourth input\n",
        "                                              static_df[[\"PC_1\",\"PC_2\",\"PC_3\",\"enc_04_v7\",\"enc_01_v1\",\"enc_07_v7\",\"enc_01_v4\",\"enc_05_v4\",\n",
        "                                                        \"enc_03_v6\",\"enc_05_v7\",\"enc_06_v6\",\"enc_01_v6\",\"enc_06_v7\",\"enc_08_v6\",\"enc_04_v6\",\n",
        "                                                         \"bg-0:00\",\"insulin_av-0:00\",\"brake-0:00\",\"intake-0:00\"]].values),\n",
        "                                              target))\n",
        "\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(batch_size)\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "jABXnHzFJ5mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataframe_to_dataset(X_train,batch_size=1)"
      ],
      "metadata": {
        "id": "QCu8MJf5J5mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.filter(regex='insulin')"
      ],
      "metadata": {
        "id": "K7Z0mheoQyYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TEST THE DATALOADER:**"
      ],
      "metadata": {
        "id": "J1aqJITPJ5mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (x0, x1, x2, x3, x4), y in train_ds.take(1):\n",
        "    print(x0.shape,x1.shape,x2.shape,x3.shape,x4.shape,y.shape)"
      ],
      "metadata": {
        "id": "_0es-IxOJ5md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.iloc[:2000,:].to_csv(\"train_sample.csv\")"
      ],
      "metadata": {
        "id": "tYzR-NvVJ5md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.2 Encoding**"
      ],
      "metadata": {
        "id": "BGOffvpmJ5md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "static_col = list(X_train.drop(columns=ts_fields+[\"bg+1:00\"]).columns)\n",
        "\n",
        "static_entries = {}\n",
        "\n",
        "for col in static_col:\n",
        "    static_entries[col] = X_train[col].nunique()\n",
        "\n",
        "embed_dim = {'cluster_pca': 3, 'hour': 8, 'minute': 4, 'cluster': 8}\n",
        "#static_entries"
      ],
      "metadata": {
        "id": "X5928jRMJ5md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_tabular(inputs, list_categorical_nn, Cat_Feat_Entries=static_entries, num_dense_exp=False, embedding_dims=embed_dim, name=\"enc\"):\n",
        "    encoded_categorical_feature_list = []\n",
        "    numerical_feature_list = []\n",
        "\n",
        "    for counter, feature_name in enumerate(inputs):\n",
        "\n",
        "      vocabulary = Cat_Feat_Entries[feature_name]\n",
        "      emb_dim = embedding_dims[feature_name]\n",
        "\n",
        "      embedding = layers.Embedding(input_dim=vocabulary, output_dim=emb_dim, name=f\"embedder_{counter}\")\n",
        "      # Convert the index values to embedding representations.\n",
        "      encoded_categorical_feature = embedding(inputs[feature_name])\n",
        "\n",
        "      encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
        "\n",
        "    return encoded_categorical_feature_list, numerical_feature_list"
      ],
      "metadata": {
        "id": "SANjLzzoJ5me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "89P6gmmbJ5me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.3 Create Models:**"
      ],
      "metadata": {
        "id": "kifxKStlJ5me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_v3(dropout=0.3,\n",
        "                    activation=\"selu\",\n",
        "                    reg=0.0,\n",
        "                    lr=0.001,\n",
        "                    base=32,\n",
        "                    rep=4,\n",
        "                    summary=False):\n",
        "\n",
        "  # --- Tabular Data Processing ---\n",
        "  # Input layer for tabular data as a dictionary\n",
        "  hour_input = keras.Input(shape=(1,), name=\"hour_input\")\n",
        "  minute_input = keras.Input(shape=(1,), name=\"minute_input\")\n",
        "  cluster_input = keras.Input(shape=(1,), name=\"cluster_input\")\n",
        "  pca_input = keras.Input(shape=(1,), name=\"cluster_pca\")\n",
        "  cont_input = keras.Input(shape=(19,), name=\"cont_inputs\")\n",
        "\n",
        "  # Lookup Layer for the p_num:\n",
        "  # pca_lookup = layers.IntegerLookup(vocabulary=list(range(0,4)),mask_token=None,num_oov_indices=0,\n",
        "  #                                  output_mode=\"int\", name=\"lookup_pca\")\n",
        "  # pca_encoded = pca_lookup(pca_input)\n",
        "\n",
        "  # Lookup Layer for the minutes:\n",
        "  minute_lookup = layers.IntegerLookup(vocabulary=list(range(0,60,5)),mask_token=None,num_oov_indices=0,\n",
        "                                       output_mode=\"int\", name=\"lookup_min\")\n",
        "\n",
        "  minute_encoded = minute_lookup(minute_input)\n",
        "\n",
        "  # Embedding layers for hour and minute\n",
        "  hour_embedding = layers.Embedding(input_dim=24, output_dim=8, name=\"embed_hour\")(hour_input)\n",
        "  minute_embedding = layers.Embedding(input_dim=12, output_dim=6, name=\"embed_minute\")(minute_encoded)\n",
        "  pca_encoded_embedding = layers.Embedding(input_dim=4,output_dim=3, name=\"embed_pca\")(pca_input)\n",
        "\n",
        "  # Embedding layer for cluster\n",
        "  cluster_embedding = layers.Embedding(input_dim=22, output_dim=8, name=\"embed_cluster\")(cluster_input)\n",
        "\n",
        "  # Flatten the embeddings\n",
        "  hour_flat = layers.Flatten()(hour_embedding)\n",
        "  minute_flat = layers.Flatten()(minute_embedding)\n",
        "  cluster_flat = layers.Flatten()(cluster_embedding)\n",
        "  pca_flat = layers.Flatten()(pca_encoded_embedding)\n",
        "\n",
        "  # Concatenate all tabular features\n",
        "  tabular_output = layers.concatenate([hour_flat, minute_flat, cluster_flat, pca_flat,cont_input],\n",
        "                                      name=\"tabular_concat\")\n",
        "  all_feat = tabular_output\n",
        "\n",
        "  # Dense layers for tabular data\n",
        "  for num, den in enumerate(range(1,rep+1)):\n",
        "    step = residual_block_dense(all_feat, base*den, activation=activation, normalization_type='batch', dropout_rate=dropout, name=f\"block_{num}\",  reg=reg,)\n",
        "    all_feat = tf.keras.layers.concatenate([all_feat,step], name=f\"concat_step_{num}\")\n",
        "\n",
        "  # --- Output Layer ---\n",
        "  # Final dense layer for prediction\n",
        "  output = layers.Dense(1, name=\"output\")(all_feat)\n",
        "\n",
        "  # --- Create and Compile the Model ---\n",
        "  # Create the model\n",
        "  model = keras.Model(inputs=[hour_input, minute_input, cluster_input, pca_input,cont_input], outputs=output)\n",
        "\n",
        "  optimizer= keras.optimizers.Adam(learning_rate=lr)\n",
        "  metric = RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
        "  metric_mae = MeanAbsoluteError(name=\"mae\", dtype=None)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[metric,metric_mae])\n",
        "\n",
        "  # Print model summary\n",
        "  if summary==True:\n",
        "    model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "#vocabulary = X_train[\"p_num\"].unique().tolist()\n",
        "model = create_model_v3(summary=True)"
      ],
      "metadata": {
        "id": "pqlqoxJdJ5me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True,\n",
        "           #rankdir=\"LR\",\n",
        "           expand_nested=True, show_layer_activations=True)"
      ],
      "metadata": {
        "id": "HkmEgbZiJ5mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Training Functions:"
      ],
      "metadata": {
        "id": "diSTHV0QJ5mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(len(val_sets)))"
      ],
      "metadata": {
        "id": "Cs2bNKDuJ5mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function:"
      ],
      "metadata": {
        "id": "kJNLakw_J5mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(X_train, X_test, model_constructor, best_params, split=val_sets, experiment_name=\"conv_v3_nn\", rs=42, target=\"bg+1:00\",\n",
        "                   batch_size=64, num_epochs=200, learning_rate=0.001, target_scaler=target_scaler):\n",
        "\n",
        "  test_predictions = np.zeros((len(X_test),1))\n",
        "  test_results_df = pd.DataFrame(index=X_test.index, columns=list(range(len(val_sets))))\n",
        "\n",
        "  all_mse = []\n",
        "  all_rmse = []\n",
        "\n",
        "  for i, val_index in enumerate(split):\n",
        "\n",
        "    print(f\"\\nRunning CV {i}\\n\")\n",
        "    ########################################################################## Prepare the Dataset:\n",
        "    X_trn = X_train.drop(index=val_index)\n",
        "    X_val = X_train.loc[val_index,:]\n",
        "\n",
        "#    vocabulary = X_trn[\"p_num\"].unique().tolist()\n",
        "\n",
        "    X = X_trn.drop(columns=[target]).copy()\n",
        "    y = X_trn[target].copy()\n",
        "\n",
        "    val_X = X_val.drop(columns=[target]).copy()\n",
        "    val_y = X_val[target].copy()\n",
        "\n",
        "    X_test = X_test.copy()\n",
        "    #################################################################### Prepare Datasets loaders:\n",
        "\n",
        "    train_dataset = dataframe_to_dataset(X_trn, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataset = dataframe_to_dataset(X_val, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    ##################################################################### Relevant Folders\n",
        "    folders_experiment = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/Glucose/{experiment_name}_{i}/\"\n",
        "    ##################################################################### Generate and Fit Model\n",
        "    # Callbacks:\n",
        "    checkpoint_filepath = folders_experiment + f'checkpoint/{experiment_name}.weights.h5'\n",
        "\n",
        "    # Generate the Model:\n",
        "    model = model_constructor(lr = learning_rate,\n",
        "                              **best_params)\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs=num_epochs,\n",
        "                        callbacks=[keras.callbacks.EarlyStopping(monitor='val_rmse', patience=17, mode=\"min\",\n",
        "                                                  start_from_epoch=5,restore_best_weights=True),\n",
        "                                   keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                    save_weights_only=True,\n",
        "                                                    monitor=\"val_rmse\",\n",
        "                                                    mode='min',\n",
        "                                                    save_best_only=True),\n",
        "                                   keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5,\n",
        "                                                          patience=5, min_lr=0.0001, mode=\"min\")],\n",
        "                        validation_data=valid_dataset)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    model.evaluate(valid_dataset, verbose=0)\n",
        "\n",
        "    plot_training_session(history)\n",
        "\n",
        "    oof_res = model.predict(valid_dataset)\n",
        "    test_pred = model.predict(test_dataset)\n",
        "\n",
        "    print(f\"Out-of-Fold Shapes: {val_y.shape},{oof_res.shape}\")\n",
        "\n",
        "    rmse_score = root_mean_squared_error(val_y, oof_res)\n",
        "\n",
        "    val_pred = target_scaler.inverse_transform(oof_res)\n",
        "    real_val_y = target_scaler.inverse_transform(val_y.values.reshape(-1,1))\n",
        "    test_pred_real = target_scaler.inverse_transform(test_pred)\n",
        "\n",
        "    rmse_score_original = root_mean_squared_error(real_val_y, val_pred)\n",
        "\n",
        "    fig, axs = plt.subplots(1,1, figsize=(10,4))\n",
        "    axs.scatter(val_pred, real_val_y)\n",
        "    axs.set_title(f\"Out-of-Fold RMSE Score: {round(rmse_score, 3)}%\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Out-of-Fold RMSE Score Rebased: {round(rmse_score, 3)}%\")\n",
        "    print(f\"Out-of-Fold RMSE Score Original: {round(rmse_score_original, 3)}%\")\n",
        "\n",
        "    ##################################################################### Save the Model\n",
        "    model.save(f\"{folders_experiment}/model_{experiment_name}.keras\")\n",
        "\n",
        "    ##################################################################### Create Model Output\n",
        "    test_results_df.loc[:,i] = test_pred_real\n",
        "    all_rmse.append(round(rmse_score_original, 3))\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "  ##################################################################### Create Model Output\n",
        "  print(f\"All Valuation RMSE: {all_rmse}\")\n",
        "\n",
        "  return test_results_df"
      ],
      "metadata": {
        "id": "n7fghvr3J5mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keras Tuner:"
      ],
      "metadata": {
        "id": "3xWPy8UkJ5mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_on=True\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "aA_Rqpx7J5mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Select a Validation set:"
      ],
      "metadata": {
        "id": "oH-7cEBXJ5mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_index = val_sets[0]\n",
        "Xt = X_train.drop(index=valid_index)\n",
        "Xv = X_train.loc[valid_index,:]\n",
        "\n",
        "print(f\"Train Shape: {Xt.shape}, Val Shape: {Xv.shape}\")"
      ],
      "metadata": {
        "id": "go3QtJYeJ5mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(Xt, batch_size=256, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(Xv, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "LBhoI5N8J5mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "512/4"
      ],
      "metadata": {
        "id": "xOSB9cPyJ5mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('activation', [\"relu\",\"silu\",\"selu\",\"gelu\"]) #\n",
        "  hp.Float('dropout',0.15,0.40, step=0.025) #\n",
        "  hp.Float('reg',0.0001, 1.0,step=10,sampling=\"log\") #\n",
        "  hp.Choice('rep', values=[3,4,5,6])\n",
        "#  hp.Choice('base', values=[32,64,128])"
      ],
      "metadata": {
        "id": "Jq18LfuCJ5mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_model_v3(dropout=hp.get('dropout'),\n",
        "                          rep=hp.get('rep'),\n",
        "                          activation=hp.get('activation'),\n",
        "                          reg=hp.get('reg'),\n",
        "                          lr=0.0025,\n",
        "                          summary=False)\n",
        "  return model"
      ],
      "metadata": {
        "id": "zsue7y_YJ5ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_rmse\", \"min\"),\n",
        "                              hyperparameters=hp, max_trials=50, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_rmse', patience=7, mode=\"min\", start_from_epoch=5)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3, min_lr=0.0001, mode=\"min\")\n",
        "\n",
        "  tuner.search(train_dataset, validation_data=valid_dataset, epochs=31, callbacks=[stop_early,reduce_])"
      ],
      "metadata": {
        "id": "I-oCVZ_aJ5ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  tuner.results_summary(num_trials=21)"
      ],
      "metadata": {
        "id": "PNj4Hy-GJ5mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)\n",
        "  print(tuner.get_best_hyperparameters(5)[4].values)\n",
        "  print(tuner.get_best_hyperparameters(6)[5].values)\n",
        "  print(tuner.get_best_hyperparameters(7)[6].values)"
      ],
      "metadata": {
        "id": "DTVF4wxCJ5mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* {'activation': 'silu', 'dropout': 0.375, 'reg': 0.0001, 'rep': 4}\n",
        "* {'activation': 'gelu', 'dropout': 0.35, 'reg': 0.001, 'rep': 4}\n",
        "* {'activation': 'gelu', 'dropout': 0.275, 'reg': 0.0001, 'rep': 4}\n",
        "* {'activation': 'gelu', 'dropout': 0.275, 'reg': 0.001, 'rep': 4}\n",
        "* {'activation': 'silu', 'dropout': 0.375, 'reg': 0.001, 'rep': 4}\n",
        "* {'activation': 'silu', 'dropout': 0.15, 'reg': 0.0001, 'rep': 3}\n",
        "* {'activation': 'silu', 'dropout': 0.375, 'reg': 0.0001, 'rep': 4}\n",
        "\n",
        "\n",
        "* **Trial 21 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "dense_layers: 512\n",
        "conv_layers: 64\n",
        "Score: 1.9809489250183105\n",
        "\n",
        "* **Trial 26 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.375\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "conv_layers: 64\n",
        "Score: 1.9827678203582764\n",
        "\n",
        "* **Trial 10 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.30000000000000004\n",
        "reg: 0.001\n",
        "dense_layers: 128\n",
        "conv_layers: 64\n",
        "Score: 1.9926979541778564\n",
        "\n",
        "* **Trial 15 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.375\n",
        "reg: 0.001\n",
        "dense_layers: 256\n",
        "conv_layers: 64\n",
        "Score: 1.9969719648361206\n",
        "\n",
        "* **Trial 18 summary**\n",
        "Hyperparameters:\n",
        "main_activation: relu\n",
        "dropout: 0.2\n",
        "reg: 0.0001\n",
        "dense_layers: 64\n",
        "conv_layers: 64\n",
        "Score: 2.0100858211517334"
      ],
      "metadata": {
        "id": "tWjiLje8J5mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit The Model:"
      ],
      "metadata": {
        "id": "rf6sIJTaJ5mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#best_params = tuner.get_best_hyperparameters(1)[0].values\n",
        "best_params = {'activation': 'silu', 'dropout': 0.375, 'reg': 0.0001, 'rep': 4}"
      ],
      "metadata": {
        "id": "vfQXTjwfJ5mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df = run_experiment(X_train, X_test, create_model_v3, best_params, split=val_sets,\n",
        "                                 experiment_name=\"conv_v3_nn\", rs=42, target=\"bg+1:00\",\n",
        "                                 batch_size=256, num_epochs=200, learning_rate=0.00025)"
      ],
      "metadata": {
        "id": "9093NTb5J5mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df.clip(2.7, 22.5, inplace=True)\n",
        "test_results_df[\"average\"] = test_results_df.mean(axis=1)\n",
        "test_results_df"
      ],
      "metadata": {
        "id": "dfwG0Ht7J5mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_res = test_results_df.clip(2.7,22.5)"
      ],
      "metadata": {
        "id": "0bF7UBY9GIs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(check_res[0],check_res[2])"
      ],
      "metadata": {
        "id": "632IC3tpFqRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **STORE RESULTS**"
      ],
      "metadata": {
        "id": "TYUCdvTbJ5mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "Ijk39Y9ZJ5mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, col in enumerate(test_results_df.columns):\n",
        "  sub[\"bg+1:00\"] = test_results_df[col].values\n",
        "  sub.to_csv(f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/submissions/Submission_val_conv_v4_{col}_clipped.csv\", index=False)\n",
        "  print(sub.isna().sum())"
      ],
      "metadata": {
        "id": "UrUexCFEJ5mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.5 MODEL 05**"
      ],
      "metadata": {
        "id": "fD45Dhe4u0nA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Create Dataloader:"
      ],
      "metadata": {
        "id": "JVvZlh2Bu0nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "4xkY1_9Mu0nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train[[\"PC_1\",\"PC_2\",\"PC_3\",\"enc_04_v7\",\"enc_01_v1\",\"enc_07_v7\",\"enc_01_v4\",\"enc_05_v4\",\n",
        "#          \"enc_03_v6\",\"enc_05_v7\",\"enc_06_v6\",\"enc_01_v6\",\"enc_06_v7\",\"enc_08_v6\",\"enc_04_v6\",\n",
        "#          \"bg-0:00\",\"insulin_av-0:00\",\"brake-0:00\",\"intake-0:00\"]].corr().style.background_gradient(cmap='coolwarm')"
      ],
      "metadata": {
        "id": "GWVH-6HcYHkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_to_dataset(dataframe, shuffle=False, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    target = dataframe[\"bg+1:00\"]\n",
        "    dataframe = dataframe.drop(columns=[\"bg+1:00\"])\n",
        "\n",
        "    static_df = dataframe.copy()\n",
        "\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((static_df[\"hour\"].values,  # First input\n",
        "                                              static_df[\"minute\"].values,  # Second input\n",
        "                                              static_df[\"cluster\"].values,  # Third input\n",
        "                                              static_df[\"cluster_pca\"].values,  # Fourth input\n",
        "                                              static_df[[\"PC_1\",\"PC_2\",\"PC_3\",\"enc_04_v7\",\"enc_01_v1\",\"enc_07_v7\",\"enc_01_v4\",\"enc_05_v4\",\n",
        "                                                        \"enc_03_v6\",\"enc_05_v7\",\"enc_06_v6\",\"enc_01_v6\",\"enc_06_v7\",\"enc_08_v6\",\"enc_04_v6\",\n",
        "                                                         \"bg-0:00\",\"insulin_av-0:00\",\"brake-0:00\",\"intake-0:00\"]].values),\n",
        "                                              target))\n",
        "\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(batch_size)\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "DJ77SK-Zu0nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataframe_to_dataset(X_train,batch_size=1)"
      ],
      "metadata": {
        "id": "4Vw8QF6Au0nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TEST THE DATALOADER:**"
      ],
      "metadata": {
        "id": "ruwAzkDWu0nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (x0, x1, x2, x3, x4), y in train_ds.take(1):\n",
        "    print(x0.shape,x1.shape,x2.shape,x3.shape,x4.shape,y.shape)"
      ],
      "metadata": {
        "id": "12uMu4VIu0nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.iloc[:2000,:].to_csv(\"train_sample.csv\")"
      ],
      "metadata": {
        "id": "40MZsI7Tu0nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.2 Encoding**"
      ],
      "metadata": {
        "id": "Pj6sE-mlu0nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "static_col = list(X_train.drop(columns=ts_fields+[\"bg+1:00\"]).columns)\n",
        "\n",
        "static_entries = {}\n",
        "\n",
        "for col in static_col:\n",
        "    static_entries[col] = X_train[col].nunique()\n",
        "\n",
        "embed_dim = {'cluster_pca': 3, 'hour': 8, 'minute': 4, 'cluster': 8}\n",
        "#static_entries"
      ],
      "metadata": {
        "id": "31qeXkmTu0nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_tabular(inputs, list_categorical_nn, Cat_Feat_Entries=static_entries, num_dense_exp=False, embedding_dims=embed_dim, name=\"enc\"):\n",
        "    encoded_categorical_feature_list = []\n",
        "    numerical_feature_list = []\n",
        "\n",
        "    for counter, feature_name in enumerate(inputs):\n",
        "\n",
        "      vocabulary = Cat_Feat_Entries[feature_name]\n",
        "      emb_dim = embedding_dims[feature_name]\n",
        "\n",
        "      embedding = layers.Embedding(input_dim=vocabulary, output_dim=emb_dim, name=f\"embedder_{counter}\")\n",
        "      # Convert the index values to embedding representations.\n",
        "      encoded_categorical_feature = embedding(inputs[feature_name])\n",
        "\n",
        "      encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
        "\n",
        "    return encoded_categorical_feature_list, numerical_feature_list"
      ],
      "metadata": {
        "id": "fn1sBLBGu0nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "SYQJa6n3u0nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.3 Create Models:**"
      ],
      "metadata": {
        "id": "cRGNsmN2u0nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_v4(dropout=0.3,\n",
        "                    activation=\"selu\",\n",
        "                    reg=0.0,\n",
        "                    lr=0.001,\n",
        "                    dense_layers=[256,128,128,64],\n",
        "                    norm=\"batch\",\n",
        "                    summary=False):\n",
        "\n",
        "  # --- Tabular Data Processing ---\n",
        "  # Input layer for tabular data as a dictionary\n",
        "  hour_input = keras.Input(shape=(1,), name=\"hour_input\")\n",
        "  minute_input = keras.Input(shape=(1,), name=\"minute_input\")\n",
        "  cluster_input = keras.Input(shape=(1,), name=\"cluster_input\")\n",
        "  pca_input = keras.Input(shape=(1,), name=\"cluster_pca\")\n",
        "  cont_input = keras.Input(shape=(19,), name=\"cont_inputs\")\n",
        "\n",
        "  # Lookup Layer for the p_num:\n",
        "  # pca_lookup = layers.IntegerLookup(vocabulary=list(range(0,4)),mask_token=None,num_oov_indices=0,\n",
        "  #                                  output_mode=\"int\", name=\"lookup_pca\")\n",
        "  # pca_encoded = pca_lookup(pca_input)\n",
        "\n",
        "  # Lookup Layer for the minutes:\n",
        "  minute_lookup = layers.IntegerLookup(vocabulary=list(range(0,60,5)),mask_token=None,num_oov_indices=0,\n",
        "                                       output_mode=\"int\", name=\"lookup_min\")\n",
        "\n",
        "  minute_encoded = minute_lookup(minute_input)\n",
        "\n",
        "  # Embedding layers for hour and minute\n",
        "  hour_embedding = layers.Embedding(input_dim=24, output_dim=8, name=\"embed_hour\")(hour_input)\n",
        "  minute_embedding = layers.Embedding(input_dim=12, output_dim=6, name=\"embed_minute\")(minute_encoded)\n",
        "  pca_encoded_embedding = layers.Embedding(input_dim=4,output_dim=3, name=\"embed_pca\")(pca_input)\n",
        "\n",
        "  # Embedding layer for cluster\n",
        "  cluster_embedding = layers.Embedding(input_dim=22, output_dim=8, name=\"embed_cluster\")(cluster_input)\n",
        "\n",
        "  # Flatten the embeddings\n",
        "  hour_flat = layers.Flatten()(hour_embedding)\n",
        "  minute_flat = layers.Flatten()(minute_embedding)\n",
        "  cluster_flat = layers.Flatten()(cluster_embedding)\n",
        "  pca_flat = layers.Flatten()(pca_encoded_embedding)\n",
        "\n",
        "  # Concatenate all tabular features\n",
        "  tabular_output = layers.concatenate([hour_flat, minute_flat, cluster_flat, pca_flat,cont_input],\n",
        "                                      name=\"tabular_concat\")\n",
        "  x = tabular_output\n",
        "\n",
        "  # Dense layers for tabular data\n",
        "  for num, den in enumerate(dense_layers):\n",
        "    x = dense_block(den, dropout=dropout, activation=activation, reg=reg, name=f\"Dense_block_{num}\", normalization_type=norm)(x)\n",
        "\n",
        "  # --- Output Layer ---\n",
        "  # Final dense layer for prediction\n",
        "  output = layers.Dense(1, name=\"output\")(x)\n",
        "\n",
        "  # --- Create and Compile the Model ---\n",
        "  # Create the model\n",
        "  model = keras.Model(inputs=[hour_input, minute_input, cluster_input, pca_input,cont_input], outputs=output)\n",
        "\n",
        "  optimizer= keras.optimizers.Adam(learning_rate=lr)\n",
        "  metric = RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
        "  metric_mae = MeanAbsoluteError(name=\"mae\", dtype=None)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[metric,metric_mae])\n",
        "\n",
        "  # Print model summary\n",
        "  if summary==True:\n",
        "    model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "#vocabulary = X_train[\"p_num\"].unique().tolist()\n",
        "model = create_model_v4(summary=True)"
      ],
      "metadata": {
        "id": "b-LMBNSZu0nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True,\n",
        "           #rankdir=\"LR\",\n",
        "           expand_nested=True, show_layer_activations=True)"
      ],
      "metadata": {
        "id": "-GeyITc_u0nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Training Functions:"
      ],
      "metadata": {
        "id": "AH4qci4Gu0nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(len(val_sets)))"
      ],
      "metadata": {
        "id": "jRhsfaW2u0nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function:"
      ],
      "metadata": {
        "id": "RTNdvUDJu0nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(X_train, X_test, model_constructor, best_params, split=val_sets, experiment_name=\"conv_v4_nn\", rs=42, target=\"bg+1:00\",\n",
        "                   batch_size=64, num_epochs=200, learning_rate=0.001, target_scaler=target_scaler):\n",
        "\n",
        "  test_predictions = np.zeros((len(X_test),1))\n",
        "  test_results_df = pd.DataFrame(index=X_test.index, columns=list(range(len(val_sets))))\n",
        "\n",
        "  all_mse = []\n",
        "  all_rmse = []\n",
        "\n",
        "  for i, val_index in enumerate(split):\n",
        "\n",
        "    print(f\"\\nRunning CV {i}\\n\")\n",
        "    ########################################################################## Prepare the Dataset:\n",
        "    X_trn = X_train.drop(index=val_index)\n",
        "    X_val = X_train.loc[val_index,:]\n",
        "\n",
        "#    vocabulary = X_trn[\"p_num\"].unique().tolist()\n",
        "\n",
        "    X = X_trn.drop(columns=[target]).copy()\n",
        "    y = X_trn[target].copy()\n",
        "\n",
        "    val_X = X_val.drop(columns=[target]).copy()\n",
        "    val_y = X_val[target].copy()\n",
        "\n",
        "    X_test = X_test.copy()\n",
        "    #################################################################### Prepare Datasets loaders:\n",
        "\n",
        "    train_dataset = dataframe_to_dataset(X_trn, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataset = dataframe_to_dataset(X_val, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    ##################################################################### Relevant Folders\n",
        "    folders_experiment = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/Glucose/{experiment_name}_{i}/\"\n",
        "    ##################################################################### Generate and Fit Model\n",
        "    # Callbacks:\n",
        "    checkpoint_filepath = folders_experiment + f'checkpoint/{experiment_name}.weights.h5'\n",
        "\n",
        "    # Generate the Model:\n",
        "    model = model_constructor(lr = learning_rate,\n",
        "                              **best_params)\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs=num_epochs,\n",
        "                        callbacks=[keras.callbacks.EarlyStopping(monitor='val_rmse', patience=17, mode=\"min\",\n",
        "                                                  start_from_epoch=5,restore_best_weights=True),\n",
        "                                   keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                    save_weights_only=True,\n",
        "                                                    monitor=\"val_rmse\",\n",
        "                                                    mode='min',\n",
        "                                                    save_best_only=True),\n",
        "                                   keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5,\n",
        "                                                          patience=5, min_lr=0.0001, mode=\"min\")],\n",
        "                        validation_data=valid_dataset)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    model.evaluate(valid_dataset, verbose=0)\n",
        "\n",
        "    plot_training_session(history)\n",
        "\n",
        "    oof_res = model.predict(valid_dataset)\n",
        "    test_pred = model.predict(test_dataset)\n",
        "\n",
        "    print(f\"Out-of-Fold Shapes: {val_y.shape},{oof_res.shape}\")\n",
        "\n",
        "    rmse_score = root_mean_squared_error(val_y, oof_res)\n",
        "\n",
        "    val_pred = target_scaler.inverse_transform(oof_res)\n",
        "    real_val_y = target_scaler.inverse_transform(val_y.values.reshape(-1,1))\n",
        "    test_pred_real = target_scaler.inverse_transform(test_pred)\n",
        "\n",
        "    rmse_score_original = root_mean_squared_error(real_val_y, val_pred)\n",
        "\n",
        "    fig, axs = plt.subplots(1,1, figsize=(10,4))\n",
        "    axs.scatter(val_pred, real_val_y)\n",
        "    axs.set_title(f\"Out-of-Fold RMSE Score: {round(rmse_score, 3)}%\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Out-of-Fold RMSE Score Rebased: {round(rmse_score, 3)}%\")\n",
        "    print(f\"Out-of-Fold RMSE Score Original: {round(rmse_score_original, 3)}%\")\n",
        "\n",
        "    ##################################################################### Save the Model\n",
        "    model.save(f\"{folders_experiment}/model_{experiment_name}.keras\")\n",
        "\n",
        "    ##################################################################### Create Model Output\n",
        "    test_results_df.loc[:,i] = test_pred_real\n",
        "    all_rmse.append(round(rmse_score_original, 3))\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "  ##################################################################### Create Model Output\n",
        "  print(f\"All Valuation RMSE: {all_rmse}\")\n",
        "\n",
        "  return test_results_df"
      ],
      "metadata": {
        "id": "exDG4i7eu0nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keras Tuner:"
      ],
      "metadata": {
        "id": "SaxMDD5Cu0nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_on=True\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "Jyn3xHUyu0nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Select a Validation set:"
      ],
      "metadata": {
        "id": "z-J0Q0uZu0nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_index = val_sets[0]\n",
        "Xt = X_train.drop(index=valid_index)\n",
        "Xv = X_train.loc[valid_index,:]\n",
        "\n",
        "print(f\"Train Shape: {Xt.shape}, Val Shape: {Xv.shape}\")"
      ],
      "metadata": {
        "id": "dBXrNaFwu0nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(Xt, batch_size=256, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(Xv, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "Tvp4e8QKu0nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "512/4"
      ],
      "metadata": {
        "id": "mgpmadHuu0nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('main_activation', [\"relu\",\"silu\",\"selu\",\"gelu\"]) #\n",
        "  hp.Float('dropout',0.15,0.40, step=0.025) #\n",
        "  hp.Float('reg',0.0001, 1.0,step=10,sampling=\"log\") #\n",
        "  hp.Choice('dense_layers', values=[512,256,128])\n",
        "  hp.Choice('norm', values=[\"batch\",\"layer\"])"
      ],
      "metadata": {
        "id": "Uhjwjio4u0nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_model_v4(dropout=hp.get('dropout'),\n",
        "                    activation=hp.get('main_activation'),\n",
        "                    reg=hp.get('reg'),\n",
        "                    lr=0.001,\n",
        "                    dense_layers=[hp.get('dense_layers'),int(hp.get('dense_layers')/2),int(hp.get('dense_layers')/2),int(hp.get('dense_layers')/4)],\n",
        "                    norm=hp.get('norm'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "QxjuCCVRu0nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_rmse\", \"min\"),\n",
        "                              hyperparameters=hp, max_trials=50, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_rmse', patience=7, mode=\"min\", start_from_epoch=5)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3, min_lr=0.0001, mode=\"min\")\n",
        "\n",
        "  tuner.search(train_dataset, validation_data=valid_dataset, epochs=31, callbacks=[stop_early,reduce_])"
      ],
      "metadata": {
        "id": "YzvS2M6ou0nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  tuner.results_summary(num_trials=21)"
      ],
      "metadata": {
        "id": "yG9u3mfAu0nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)"
      ],
      "metadata": {
        "id": "eB1fTieAu0nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* {'main_activation': 'gelu', 'dropout': 0.2, 'reg': 0.001, 'dense_layers': 128, 'norm': 'batch'}\n",
        "* {'main_activation': 'gelu', 'dropout': 0.175, 'reg': 0.001, 'dense_layers': 128, 'norm': 'batch'}\n",
        "* {'main_activation': 'silu', 'dropout': 0.35, 'reg': 0.001, 'dense_layers': 256, 'norm': 'batch'}\n",
        "* {'main_activation': 'silu', 'dropout': 0.35, 'reg': 0.001, 'dense_layers': 512, 'norm': 'batch'}\n",
        "\n",
        "* **Trial 49 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.2\n",
        "reg: 0.001\n",
        "dense_layers: 128\n",
        "norm: batch\n",
        "Score: 0.6541268825531006\n",
        "\n",
        "* **Trial 23 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.175\n",
        "reg: 0.001\n",
        "dense_layers: 128\n",
        "norm: batch\n",
        "Score: 0.6549822688102722\n",
        "\n",
        "* **Trial 05 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.35\n",
        "reg: 0.001\n",
        "dense_layers: 256\n",
        "norm: batch\n",
        "Score: 0.6553966403007507\n",
        "\n",
        "* **Trial 25 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.35\n",
        "reg: 0.001\n",
        "dense_layers: 512\n",
        "norm: batch\n",
        "Score: 0.6563018560409546\n",
        "\n",
        "* **Trial 12 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.225\n",
        "reg: 0.001\n",
        "dense_layers: 128\n",
        "norm: batch\n",
        "Score: 0.6568679213523865\n",
        "\n",
        "* **Trial 48 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.325\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "norm: layer\n",
        "Score: 0.6574869155883789\n",
        "\n",
        "* **Trial 17 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.30000000000000004\n",
        "reg: 0.1\n",
        "dense_layers: 512\n",
        "norm: batch\n",
        "Score: 0.6574987769126892\n",
        "\n",
        "* **Trial 26 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.2\n",
        "reg: 0.01\n",
        "dense_layers: 128\n",
        "norm: batch\n",
        "Score: 0.6576610207557678"
      ],
      "metadata": {
        "id": "FQ6wajJGu0nN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit The Model:"
      ],
      "metadata": {
        "id": "GIIOuDvgu0nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#best_params = tuner.get_best_hyperparameters(1)[0].values\n",
        "best_params = {'activation': 'gelu', 'dropout': 0.2, 'reg': 0.001, 'dense_layers': [128,64,64,32],'norm': 'batch'}"
      ],
      "metadata": {
        "id": "z90luma4u0nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df = run_experiment(X_train, X_test, create_model_v4, best_params, split=val_sets,\n",
        "                                 experiment_name=\"dnn_v0\", rs=42, target=\"bg+1:00\",\n",
        "                                 batch_size=256, num_epochs=200, learning_rate=0.00025)"
      ],
      "metadata": {
        "id": "EZh9015Ju0nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df.clip(2.7, 22.5, inplace=True)\n",
        "test_results_df[\"average\"] = test_results_df.mean(axis=1)\n",
        "test_results_df"
      ],
      "metadata": {
        "id": "WoQDGWZju0nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **STORE RESULTS**"
      ],
      "metadata": {
        "id": "bWicG1kGu0nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "j4VVtBdDu0nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, col in enumerate(test_results_df.columns):\n",
        "  sub[\"bg+1:00\"] = test_results_df[col].values\n",
        "  sub.to_csv(f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/submissions/Submission_val_dnn_v0_{col}_clipped.csv\", index=False)\n",
        "  print(sub.isna().sum())"
      ],
      "metadata": {
        "id": "GJW6B0wOu0nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df.min(axis=0),test_results_df.max(axis=0)"
      ],
      "metadata": {
        "id": "Nb4rEmECbPiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(test_results_df[0],test_results_df[1])"
      ],
      "metadata": {
        "id": "idUFLQMZbPeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.6 MODEL 06**"
      ],
      "metadata": {
        "id": "3Wn3TmNpF8vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Create Dataloader:"
      ],
      "metadata": {
        "id": "Z67f6rHfF8vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "DiUvsOJjF8vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train[[\"PC_1\",\"PC_2\",\"PC_3\",\"enc_04_v7\",\"enc_01_v1\",\"enc_07_v7\",\"enc_01_v4\",\"enc_05_v4\",\n",
        "#          \"enc_03_v6\",\"enc_05_v7\",\"enc_06_v6\",\"enc_01_v6\",\"enc_06_v7\",\"enc_08_v6\",\"enc_04_v6\",\n",
        "#          \"bg-0:00\",\"insulin_av-0:00\",\"brake-0:00\",\"intake-0:00\"]].corr().style.background_gradient(cmap='coolwarm')"
      ],
      "metadata": {
        "id": "0RqKEMQvF8vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_to_dataset(dataframe, shuffle=False, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    target = dataframe[\"bg+1:00\"]\n",
        "    dataframe = dataframe.drop(columns=[\"bg+1:00\"])\n",
        "\n",
        "    static_df = dataframe.copy()\n",
        "\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((static_df[\"hour\"].values,  # First input\n",
        "                                              static_df[\"minute\"].values,  # Second input\n",
        "                                              static_df[\"cluster\"].values,  # Third input\n",
        "                                              static_df[\"cluster_pca\"].values,  # Fourth input\n",
        "                                              static_df[[\"PC_1\",\"PC_2\",\"PC_3\",\"enc_04_v7\",\"enc_01_v1\",\"enc_07_v7\",\"enc_01_v4\",\"enc_05_v4\",\n",
        "                                                        \"enc_03_v6\",\"enc_05_v7\",\"enc_06_v6\",\"enc_01_v6\",\"enc_06_v7\",\"enc_08_v6\",\"enc_04_v6\",\n",
        "                                                         \"bg-0:00\",\"insulin_av-0:00\",\"brake-0:00\",\"intake-0:00\"]].values),\n",
        "                                              target))\n",
        "\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(batch_size)\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "t9vR91BuF8vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataframe_to_dataset(X_train,batch_size=1)"
      ],
      "metadata": {
        "id": "NcSIbCy6F8vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TEST THE DATALOADER:**"
      ],
      "metadata": {
        "id": "Qs0NOVv3F8vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (x0, x1, x2, x3, x4), y in train_ds.take(1):\n",
        "    print(x0.shape,x1.shape,x2.shape,x3.shape,x4.shape,y.shape)"
      ],
      "metadata": {
        "id": "JM8nKh4kF8vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.iloc[:2000,:].to_csv(\"train_sample.csv\")"
      ],
      "metadata": {
        "id": "48c7bHBjF8vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.2 Encoding**"
      ],
      "metadata": {
        "id": "VsykdYR5F8vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "static_col = list(X_train.drop(columns=ts_fields+[\"bg+1:00\"]).columns)\n",
        "\n",
        "static_entries = {}\n",
        "\n",
        "for col in static_col:\n",
        "    static_entries[col] = X_train[col].nunique()\n",
        "\n",
        "embed_dim = {'cluster_pca': 3, 'hour': 8, 'minute': 4, 'cluster': 8}\n",
        "#static_entries"
      ],
      "metadata": {
        "id": "uS41S57jF8vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_tabular(inputs, list_categorical_nn, Cat_Feat_Entries=static_entries, num_dense_exp=False, embedding_dims=embed_dim, name=\"enc\"):\n",
        "    encoded_categorical_feature_list = []\n",
        "    numerical_feature_list = []\n",
        "\n",
        "    for counter, feature_name in enumerate(inputs):\n",
        "\n",
        "      vocabulary = Cat_Feat_Entries[feature_name]\n",
        "      emb_dim = embedding_dims[feature_name]\n",
        "\n",
        "      embedding = layers.Embedding(input_dim=vocabulary, output_dim=emb_dim, name=f\"embedder_{counter}\")\n",
        "      # Convert the index values to embedding representations.\n",
        "      encoded_categorical_feature = embedding(inputs[feature_name])\n",
        "\n",
        "      encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
        "\n",
        "    return encoded_categorical_feature_list, numerical_feature_list"
      ],
      "metadata": {
        "id": "6LBfEJuWF8vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "VKnT9vVtF8vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.3 Create Models:**"
      ],
      "metadata": {
        "id": "fFRlkRoMF8vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_v5(dropout=0.3,\n",
        "                    activation=\"selu\",\n",
        "                    reg=0.0,\n",
        "                    lr=0.001,\n",
        "                    dense_layers=[256,128,128,64],\n",
        "                    norm=\"batch\",\n",
        "                    summary=False):\n",
        "\n",
        "  # --- Tabular Data Processing ---\n",
        "  # Input layer for tabular data as a dictionary\n",
        "  hour_input = keras.Input(shape=(1,), name=\"hour_input\")\n",
        "  minute_input = keras.Input(shape=(1,), name=\"minute_input\")\n",
        "  cluster_input = keras.Input(shape=(1,), name=\"cluster_input\")\n",
        "  pca_input = keras.Input(shape=(1,), name=\"cluster_pca\")\n",
        "  cont_input = keras.Input(shape=(19,), name=\"cont_inputs\")\n",
        "\n",
        "  # Lookup Layer for the p_num:\n",
        "  # pca_lookup = layers.IntegerLookup(vocabulary=list(range(0,4)),mask_token=None,num_oov_indices=0,\n",
        "  #                                  output_mode=\"int\", name=\"lookup_pca\")\n",
        "  # pca_encoded = pca_lookup(pca_input)\n",
        "\n",
        "  # Lookup Layer for the minutes:\n",
        "  minute_lookup = layers.IntegerLookup(vocabulary=list(range(0,60,5)),mask_token=None,num_oov_indices=0,\n",
        "                                       output_mode=\"int\", name=\"lookup_min\")\n",
        "\n",
        "  minute_encoded = minute_lookup(minute_input)\n",
        "\n",
        "  # Embedding layers for hour and minute\n",
        "  hour_embedding = layers.Embedding(input_dim=24, output_dim=8, name=\"embed_hour\")(hour_input)\n",
        "  minute_embedding = layers.Embedding(input_dim=12, output_dim=6, name=\"embed_minute\")(minute_encoded)\n",
        "  pca_encoded_embedding = layers.Embedding(input_dim=4,output_dim=3, name=\"embed_pca\")(pca_input)\n",
        "\n",
        "  # Embedding layer for cluster\n",
        "  cluster_embedding = layers.Embedding(input_dim=22, output_dim=8, name=\"embed_cluster\")(cluster_input)\n",
        "\n",
        "  # Flatten the embeddings\n",
        "  hour_flat = layers.Flatten()(hour_embedding)\n",
        "  minute_flat = layers.Flatten()(minute_embedding)\n",
        "  cluster_flat = layers.Flatten()(cluster_embedding)\n",
        "  pca_flat = layers.Flatten()(pca_encoded_embedding)\n",
        "\n",
        "  # Concatenate all tabular features\n",
        "  x0 = layers.concatenate([hour_flat, minute_flat, cluster_flat, pca_flat,cont_input],\n",
        "                                      name=\"tabular_concat\")\n",
        "  cross = x0\n",
        "  for _ in dense_layers:\n",
        "      units = cross.shape[-1]\n",
        "      x = layers.Dense(units)(cross)\n",
        "      cross = x0 * x + cross\n",
        "  cross = layers.BatchNormalization()(cross)\n",
        "\n",
        "\n",
        "  deep = x0\n",
        "  for num, units in enumerate(dense_layers):\n",
        "      deep = dense_block(units, dropout=dropout, activation=activation, reg=reg, name=f\"Dense_block_{num}\", normalization_type=norm)(deep)\n",
        "\n",
        "  merged = layers.concatenate([cross, deep])\n",
        "\n",
        "  # --- Output Layer ---\n",
        "  # Final dense layer for prediction\n",
        "  output = layers.Dense(1, name=\"output\")(merged)\n",
        "\n",
        "  # --- Create and Compile the Model ---\n",
        "  # Create the model\n",
        "  model = keras.Model(inputs=[hour_input, minute_input, cluster_input, pca_input,cont_input], outputs=output)\n",
        "\n",
        "  optimizer= keras.optimizers.Adam(learning_rate=lr)\n",
        "  metric = RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
        "  metric_mae = MeanAbsoluteError(name=\"mae\", dtype=None)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[metric,metric_mae])\n",
        "\n",
        "  # Print model summary\n",
        "  if summary==True:\n",
        "    model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "#vocabulary = X_train[\"p_num\"].unique().tolist()\n",
        "model = create_model_v5(summary=True)"
      ],
      "metadata": {
        "id": "phhmMe4fF8vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True,\n",
        "           rankdir=\"LR\",\n",
        "           expand_nested=True, show_layer_activations=True)"
      ],
      "metadata": {
        "id": "yW08xTiFF8vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Training Functions:"
      ],
      "metadata": {
        "id": "gpm7-E3HF8vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(len(val_sets)))"
      ],
      "metadata": {
        "id": "IWZg_aSBF8vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function:"
      ],
      "metadata": {
        "id": "v6MY0qKRF8vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(X_train, X_test, model_constructor, best_params, split=val_sets, experiment_name=\"conv_v5_nn\", rs=42, target=\"bg+1:00\",\n",
        "                   batch_size=64, num_epochs=200, learning_rate=0.001, target_scaler=target_scaler):\n",
        "\n",
        "  test_predictions = np.zeros((len(X_test),1))\n",
        "  test_results_df = pd.DataFrame(index=X_test.index, columns=list(range(len(val_sets))))\n",
        "\n",
        "  all_mse = []\n",
        "  all_rmse = []\n",
        "\n",
        "  for i, val_index in enumerate(split):\n",
        "\n",
        "    print(f\"\\nRunning CV {i}\\n\")\n",
        "    ########################################################################## Prepare the Dataset:\n",
        "    X_trn = X_train.drop(index=val_index)\n",
        "    X_val = X_train.loc[val_index,:]\n",
        "\n",
        "#    vocabulary = X_trn[\"p_num\"].unique().tolist()\n",
        "\n",
        "    X = X_trn.drop(columns=[target]).copy()\n",
        "    y = X_trn[target].copy()\n",
        "\n",
        "    val_X = X_val.drop(columns=[target]).copy()\n",
        "    val_y = X_val[target].copy()\n",
        "\n",
        "    X_test = X_test.copy()\n",
        "    #################################################################### Prepare Datasets loaders:\n",
        "\n",
        "    train_dataset = dataframe_to_dataset(X_trn, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataset = dataframe_to_dataset(X_val, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    ##################################################################### Relevant Folders\n",
        "    folders_experiment = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/Glucose/{experiment_name}_{i}/\"\n",
        "    ##################################################################### Generate and Fit Model\n",
        "    # Callbacks:\n",
        "    checkpoint_filepath = folders_experiment + f'checkpoint/{experiment_name}.weights.h5'\n",
        "\n",
        "    # Generate the Model:\n",
        "    model = model_constructor(lr = learning_rate,\n",
        "                              **best_params)\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs=num_epochs,\n",
        "                        callbacks=[keras.callbacks.EarlyStopping(monitor='val_rmse', patience=17, mode=\"min\",\n",
        "                                                  start_from_epoch=5,restore_best_weights=True),\n",
        "                                   keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                    save_weights_only=True,\n",
        "                                                    monitor=\"val_rmse\",\n",
        "                                                    mode='min',\n",
        "                                                    save_best_only=True),\n",
        "                                   keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5,\n",
        "                                                          patience=5, min_lr=0.0001, mode=\"min\")],\n",
        "                        validation_data=valid_dataset)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    model.evaluate(valid_dataset, verbose=0)\n",
        "\n",
        "    plot_training_session(history)\n",
        "\n",
        "    oof_res = model.predict(valid_dataset)\n",
        "    test_pred = model.predict(test_dataset)\n",
        "\n",
        "    print(f\"Out-of-Fold Shapes: {val_y.shape},{oof_res.shape}\")\n",
        "\n",
        "    rmse_score = root_mean_squared_error(val_y, oof_res)\n",
        "\n",
        "    val_pred = target_scaler.inverse_transform(oof_res)\n",
        "    real_val_y = target_scaler.inverse_transform(val_y.values.reshape(-1,1))\n",
        "    test_pred_real = target_scaler.inverse_transform(test_pred)\n",
        "\n",
        "    rmse_score_original = root_mean_squared_error(real_val_y, val_pred)\n",
        "\n",
        "    fig, axs = plt.subplots(1,1, figsize=(10,4))\n",
        "    axs.scatter(val_pred, real_val_y)\n",
        "    axs.set_title(f\"Out-of-Fold RMSE Score: {round(rmse_score, 3)}%\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Out-of-Fold RMSE Score Rebased: {round(rmse_score, 3)}%\")\n",
        "    print(f\"Out-of-Fold RMSE Score Original: {round(rmse_score_original, 3)}%\")\n",
        "\n",
        "    ##################################################################### Save the Model\n",
        "    model.save(f\"{folders_experiment}/model_{experiment_name}.keras\")\n",
        "\n",
        "    ##################################################################### Create Model Output\n",
        "    test_results_df.loc[:,i] = test_pred_real\n",
        "    all_rmse.append(round(rmse_score_original, 3))\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "  ##################################################################### Create Model Output\n",
        "  print(f\"All Valuation RMSE: {all_rmse}\")\n",
        "\n",
        "  return test_results_df"
      ],
      "metadata": {
        "id": "yx5LTRJBF8vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keras Tuner:"
      ],
      "metadata": {
        "id": "3MDKtHyhF8vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_on=True\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "2D1nzpwhF8vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Select a Validation set:"
      ],
      "metadata": {
        "id": "VwSzWXdtF8vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_index = val_sets[0]\n",
        "Xt = X_train.drop(index=valid_index)\n",
        "Xv = X_train.loc[valid_index,:]\n",
        "\n",
        "print(f\"Train Shape: {Xt.shape}, Val Shape: {Xv.shape}\")"
      ],
      "metadata": {
        "id": "5PiioM9NF8vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(Xt, batch_size=256, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(Xv, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "nQzgjTkqF8vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('activation', [\"relu\",\"silu\",\"gelu\",\"mish\"]) #\n",
        "  hp.Float('dropout',0.25,0.40, step=0.025) #\n",
        "  hp.Float('reg',0.0001, 0.1,step=10,sampling=\"log\") #\n",
        "  hp.Choice('dense_layers', values=[1028,512,256])\n",
        "  hp.Choice('norm', values=[\"batch\"])"
      ],
      "metadata": {
        "id": "5_Iu0ZpFF8vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_model_v5(dropout=hp.get('dropout'),\n",
        "                    activation=hp.get('activation'),\n",
        "                    reg=hp.get('reg'),\n",
        "                    lr=0.0005,\n",
        "                    dense_layers=[hp.get('dense_layers'),int(hp.get('dense_layers')/2),int(hp.get('dense_layers')/4)],\n",
        "                    norm=hp.get('norm'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "qZJ3GA3FF8vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_rmse\", \"min\"),\n",
        "                              hyperparameters=hp, max_trials=50, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_rmse', patience=7, mode=\"min\", start_from_epoch=5)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3, min_lr=0.0001, mode=\"min\")\n",
        "\n",
        "  tuner.search(train_dataset, validation_data=valid_dataset, epochs=31, callbacks=[stop_early,reduce_])"
      ],
      "metadata": {
        "id": "J3zsapmQF8vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Trial 19 Complete [00h 02m 35s]\n",
        "    val_rmse: 0.6537328958511353\n",
        "\n",
        "    Best val_rmse So Far: 0.6537328958511353\n",
        "    Total elapsed time: 00h 39m 28s\n",
        "\n",
        "    Search: Running Trial #20\n",
        "\n",
        "    Value             |Best Value So Far |Hyperparameter\n",
        "    gelu              |relu              |activation\n",
        "    0.225             |0.325             |dropout\n",
        "    0.0001            |0.01              |reg\n",
        "    512               |512               |dense_layers\n",
        "    batch             |batch             |norm"
      ],
      "metadata": {
        "id": "_7mmja8OVbWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  tuner.results_summary(num_trials=21)"
      ],
      "metadata": {
        "id": "EHy6TOO6F8vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)"
      ],
      "metadata": {
        "id": "soil_dY-F8vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* {'main_activation': 'gelu', 'dropout': 0.2, 'reg': 0.001, 'dense_layers': 128, 'norm': 'batch'}\n",
        "* {'main_activation': 'gelu', 'dropout': 0.175, 'reg': 0.001, 'dense_layers': 128, 'norm': 'batch'}\n",
        "* {'main_activation': 'silu', 'dropout': 0.35, 'reg': 0.001, 'dense_layers': 256, 'norm': 'batch'}\n",
        "* {'main_activation': 'silu', 'dropout': 0.35, 'reg': 0.001, 'dense_layers': 512, 'norm': 'batch'}\n",
        "\n",
        "* **Trial 49 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.2\n",
        "reg: 0.001\n",
        "dense_layers: 128\n",
        "norm: batch\n",
        "Score: 0.6541268825531006\n",
        "\n",
        "* **Trial 23 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.175\n",
        "reg: 0.001\n",
        "dense_layers: 128\n",
        "norm: batch\n",
        "Score: 0.6549822688102722\n",
        "\n",
        "* **Trial 05 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.35\n",
        "reg: 0.001\n",
        "dense_layers: 256\n",
        "norm: batch\n",
        "Score: 0.6553966403007507\n",
        "\n",
        "* **Trial 25 summary**\n",
        "Hyperparameters:\n",
        "main_activation: silu\n",
        "dropout: 0.35\n",
        "reg: 0.001\n",
        "dense_layers: 512\n",
        "norm: batch\n",
        "Score: 0.6563018560409546\n",
        "\n",
        "* **Trial 12 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.225\n",
        "reg: 0.001\n",
        "dense_layers: 128\n",
        "norm: batch\n",
        "Score: 0.6568679213523865\n",
        "\n",
        "* **Trial 48 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.325\n",
        "reg: 0.0001\n",
        "dense_layers: 128\n",
        "norm: layer\n",
        "Score: 0.6574869155883789\n",
        "\n",
        "* **Trial 17 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.30000000000000004\n",
        "reg: 0.1\n",
        "dense_layers: 512\n",
        "norm: batch\n",
        "Score: 0.6574987769126892\n",
        "\n",
        "* **Trial 26 summary**\n",
        "Hyperparameters:\n",
        "main_activation: gelu\n",
        "dropout: 0.2\n",
        "reg: 0.01\n",
        "dense_layers: 128\n",
        "norm: batch\n",
        "Score: 0.6576610207557678"
      ],
      "metadata": {
        "id": "IEej7sUbF8vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit The Model:"
      ],
      "metadata": {
        "id": "lYvZ4QWxF8vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#best_params = tuner.get_best_hyperparameters(1)[0].values\n",
        "best_params = {'activation': 'silu', 'dropout': 0.375, 'reg': 0.01, 'dense_layers': [512,256,128], 'norm': 'batch'}"
      ],
      "metadata": {
        "id": "zt39jIRfF8vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df = run_experiment(X_train, X_test, create_model_v5, best_params, split=val_sets,\n",
        "                                 experiment_name=\"dnn_v0\", rs=42, target=\"bg+1:00\",\n",
        "                                 batch_size=256, num_epochs=200, learning_rate=0.00025)"
      ],
      "metadata": {
        "id": "lfCB_jmbF8vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df.clip(2.7, 22.5, inplace=True)\n",
        "test_results_df[\"average\"] = test_results_df.mean(axis=1)\n",
        "test_results_df"
      ],
      "metadata": {
        "id": "2NWcD9BnF8vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **STORE RESULTS**"
      ],
      "metadata": {
        "id": "DCiPd4B1F8vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "rmAydRQrF8vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, col in enumerate(test_results_df.columns):\n",
        "  sub[\"bg+1:00\"] = test_results_df[col].values\n",
        "  sub.to_csv(f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/Glucose/submissions/Submission_val_crossdeep_v0_{col}_clipped.csv\", index=False)\n",
        "  print(sub.isna().sum())"
      ],
      "metadata": {
        "id": "3uSCx6-rF8vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df.min(axis=0),test_results_df.max(axis=0)"
      ],
      "metadata": {
        "id": "USZaUaEZF8vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(test_results_df[0],test_results_df[1])"
      ],
      "metadata": {
        "id": "JC2gcaMPF8vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Ey1vbXcbPZM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}